{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3332cb09",
   "metadata": {},
   "source": [
    "\n",
    "# Chapter 2: OSTEP ì „ì²˜ë¦¬ ë° ì²­í¬ JSON ì €ì¥\n",
    " \n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ OSTEP êµì¬ PDFë¥¼ ì „ì²˜ë¦¬í•˜ê³  í† í° ê¸°ë°˜ ì²­í‚¹ í›„ JSONìœ¼ë¡œ ì €ì¥í•˜ëŠ” ì „ì²´ ê³¼ì •ì„ ë‹¨ê³„ë³„ë¡œ ì‹¤ìŠµí•©ë‹ˆë‹¤.\n",
    "\n",
    "## ğŸ“š í•™ìŠµ ëª©í‘œ\n",
    "- PDF â†’ í…ìŠ¤íŠ¸ ì¶”ì¶œ íŒŒì´í”„ë¼ì¸ êµ¬ì„± ë° í’ˆì§ˆ ê´€ë¦¬ í¬ì¸íŠ¸ ì´í•´\n",
    "- ëª©ì°¨(ì±•í„°/íŒŒíŠ¸/ì„œë¸Œì„¹ì…˜) íŒŒì‹±ê³¼ í˜ì´ì§€ ë²”ìœ„ ê³„ì‚° ë°©ë²• ìŠµë“\n",
    "- ë¬¸ì¥ ê²½ê³„ ìœ ì§€í˜• í† í° ê¸°ë°˜ ì²­í‚¹ê³¼ ì˜¤ë²„ë© ì„¤ê³„ ì´í•´\n",
    "\n",
    "## ğŸ“‹ ì‹¤ìŠµ êµ¬ì„±\n",
    "- 1ï¸âƒ£ í™˜ê²½ ì„¤ì •: íŒ¨í‚¤ì§€ ì„¤ì¹˜, ê²½ë¡œ/ìƒìˆ˜ ì •ì˜, ë°ì´í„° ìœ„ì¹˜ í™•ì¸\n",
    "- 2ï¸âƒ£ ì„í¬íŠ¸/ìƒìˆ˜: ì •ê·œì‹ íŒ¨í„´, í† í°/ì˜¤ë²„ë©, ë¶„ë¦¬ ê·œì¹™ ë“± ì„¤ì •\n",
    "- 3ï¸âƒ£ PDF ë¡œë“œ/ê¸°ë³¸ í•¨ìˆ˜: í˜ì´ì§€ ì•ˆì „ ì¶”ì¶œ ìœ í‹¸ë¦¬í‹° ì¤€ë¹„\n",
    "- 4ï¸âƒ£ ëª©ì°¨ íŒŒì‹±: ì±•í„°/íŒŒíŠ¸/ì„œë¸Œì„¹ì…˜ êµ¬ì¡°í™”\n",
    "- 5ï¸âƒ£ ë²”ìœ„ ê³„ì‚°/í…ìŠ¤íŠ¸ ì¶”ì¶œ: ì±•í„°ë³„ ë²”ìœ„ ì‚°ì¶œ ë° ì •ì œ\n",
    "- 6ï¸âƒ£ ì²­í¬ ìƒì„±/ì €ì¥: ë¬¸ì¥ ê²½ê³„ ìœ ì§€í˜• í† í° ì²­í‚¹ â†’ JSON ì €ì¥\n",
    "- 7ï¸âƒ£ ê²°ê³¼ ìš”ì•½: ë²”ìœ„/ë¬¸ììˆ˜/ì„œë¸Œì„¹ì…˜ í†µê³„ ì¶œë ¥\n",
    "\n",
    "> âš ï¸ ì‹¤ìŠµ ì…€ ì‹¤í–‰ ì „, í™˜ê²½ ì„¤ì • ì…€(1ï¸âƒ£)ì„ ë¨¼ì € ì‹¤í–‰í•˜ê³  OSTEP PDF ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc101372",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## 1ï¸âƒ£ Google Colab í™˜ê²½ ì„¤ì •\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ **Google Colabì—ì„œ GPUë¥¼ ì‚¬ìš©**í•˜ì—¬ ì‹¤í–‰í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "### ğŸ“Œ ì‹¤í–‰ ì „ ì¤€ë¹„ì‚¬í•­\n",
    "1. **ëŸ°íƒ€ì„ ìœ í˜• ì„¤ì •**: ë©”ë‰´ì—ì„œ `ëŸ°íƒ€ì„` â†’ `ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½` â†’ `GPU` ì„ íƒ\n",
    "2. **ì²« ë²ˆì§¸ ì½”ë“œ ì…€ ì‹¤í–‰**: Google Drive ë§ˆìš´íŠ¸ ë° í•„ìˆ˜ íŒ¨í‚¤ì§€ ìë™ ì„¤ì¹˜\n",
    "3. **OSTEP PDF ì—…ë¡œë“œ**: ìµœì´ˆ 1íšŒë§Œ `/content/drive/MyDrive/ostep_rag/data/documents/ostep.pdf` ìœ„ì¹˜ì— ì—…ë¡œë“œ\n",
    "\n",
    "> âš ï¸ **ì¤‘ìš”**: ì•„ë˜ ì½”ë“œ ì…€ì„ ê°€ì¥ ë¨¼ì € ì‹¤í–‰í•˜ì—¬ í™˜ê²½ì„ ì„¤ì •í•˜ì„¸ìš”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colab_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Google Colab í™˜ê²½ ì„¤ì •\n",
    "# ========================================\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Google Drive ë§ˆìš´íŠ¸\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# í•„ìš” íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "!pip -q install PyPDF2 pdfplumber pymupdf tiktoken\n",
    "\n",
    "# ê²½ë¡œ ì„¤ì • (Colab ì „ìš©)\n",
    "BASE_DIR = \"/content/drive/MyDrive/ostep_rag\"\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "DOC_ID = \"ostep\"\n",
    "OUTPUT_DIR = os.path.join(DATA_DIR, \"chunk\")\n",
    "PDF_PATH = os.path.join(DATA_DIR, \"documents\", \"ostep.pdf\")\n",
    "\n",
    "# ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "os.makedirs(os.path.join(DATA_DIR, \"documents\"), exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# OSTEP PDF ì—…ë¡œë“œ í™•ì¸\n",
    "if not os.path.exists(PDF_PATH):\n",
    "    print(f\"âš ï¸ êµ¬ê¸€ ë“œë¼ì´ë¸Œì— OSTEP PDFë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”:\")\n",
    "    print(f\"   {PDF_PATH}\")\n",
    "    print(f\"\\nğŸ“ ì¢Œì¸¡ íŒŒì¼ íƒ­ â†’ drive â†’ MyDrive â†’ ostep_rag â†’ data â†’ documents í´ë”ì— ostep.pdf ì—…ë¡œë“œ\")\n",
    "else:\n",
    "    print(f\"âœ… OSTEP PDF íŒŒì¼ í™•ì¸ë¨: {PDF_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14830a1",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## 2ï¸âƒ£ ì„í¬íŠ¸ ë° ì„¤ì • ìƒìˆ˜\n",
    "\n",
    "ì´ ì…€ì—ì„œëŠ” í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„í¬íŠ¸í•˜ê³  ì „ì—­ ì„¤ì • ìƒìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "**ì£¼ìš” ë‚´ìš©:**\n",
    "- PyPDF2ë¥¼ ì‚¬ìš©í•˜ì—¬ PDF íŒŒì¼ ì²˜ë¦¬\n",
    "- ì •ê·œì‹ íŒ¨í„´ì„ ì‚¬ìš©í•˜ì—¬ ëª©ì°¨ì—ì„œ ì±•í„°, íŒŒíŠ¸, ì„œë¸Œì„¹ì…˜ íŒŒì‹±\n",
    "- í† í° ê¸°ë°˜ ì²­í‚¹ ì„¤ì •(ìµœëŒ€ í† í°, ì˜¤ë²„ë© í† í°)\n",
    "- ì¶œë ¥ ë””ë ‰í† ë¦¬ ë° ë¬¸ì„œ ID ì„¤ì •\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba277ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from PyPDF2 import PdfReader\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ì„¤ì • ìƒìˆ˜\n",
    "TOC_START_PAGE = 15\n",
    "TOC_END_PAGE = 25\n",
    "PAGE_OFFSET = 36\n",
    "CHUNK_MAX_TOKENS = 400\n",
    "CHUNK_OVERLAP_TOKENS = 80\n",
    "SENTENCE_SPLIT_REGEX = r'(?<=[.!?])\\s+(?=[A-Z0-9])'\n",
    "\n",
    "# ì •ê·œì‹ íŒ¨í„´\n",
    "MAIN_CHAPTER_PATTERNS = [\n",
    "    r'^(\\d+)\\s+(.+?)\\s+\\.{3,}\\s*(\\d+)$',\n",
    "    r'^(\\d+)\\s+(.+?)\\s+(\\d+)$'\n",
    "]\n",
    "\n",
    "PART_PATTERNS = [\n",
    "    r'^([IVX]+)\\s+(.+?)\\s+\\.{3,}\\s*(\\d+)$',\n",
    "    r'^([IVX]+)\\s+(.+?)\\s+(\\d+)$'\n",
    "]\n",
    "\n",
    "SUBSECTION_PATTERNS = [\n",
    "    r'^(\\d+\\.\\d+)\\s+(.+?)\\s+\\.{3,}\\s*(\\d+)$',\n",
    "    r'^(\\d+\\.\\d+)\\s+(.+?)\\s+(\\d+)$'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2b647b",
   "metadata": {},
   "source": [
    "---\n",
    "## 3ï¸âƒ£ PDF ë¡œë“œ ë° ê¸°ë³¸ í•¨ìˆ˜\n",
    "\n",
    "ì´ ì…€ì—ì„œëŠ” PDF íŒŒì¼ì„ ë¡œë“œí•˜ê³  ê¸°ë³¸ì ì¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "**ì£¼ìš” í•¨ìˆ˜:**\n",
    "- `load_pdf()`: PDF íŒŒì¼ì„ ë¡œë“œí•˜ê³  í˜ì´ì§€ ìˆ˜ë¥¼ í™•ì¸\n",
    "- `extract_pdf_text()`: ì§€ì •ëœ í˜ì´ì§€ ë²”ìœ„ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œ\n",
    "\n",
    "**ì‹¤í–‰ ê²°ê³¼:**\n",
    "- PDF íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ê³  ì „ì²´ í˜ì´ì§€ ìˆ˜ê°€ ì¶œë ¥ë©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053adb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(pdf_path: str):\n",
    "    \"\"\"PDF íŒŒì¼ ë¡œë“œ\"\"\"\n",
    "    pdf_path = Path(pdf_path)\n",
    "    if not pdf_path.exists():\n",
    "        raise FileNotFoundError(f\"PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_path}\")\n",
    "    \n",
    "    reader = PdfReader(pdf_path)\n",
    "    print(f\"PDF ë¡œë“œ ì™„ë£Œ: {len(reader.pages)} í˜ì´ì§€\")\n",
    "    return reader\n",
    "\n",
    "def extract_pdf_text(reader, start_page: int, end_page: int):\n",
    "    \"\"\"PDF í˜ì´ì§€ ë²”ìœ„ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ\"\"\"\n",
    "    total_pages = len(reader.pages)\n",
    "    if start_page < 1 or end_page > total_pages:\n",
    "        start_page = max(1, start_page)\n",
    "        end_page = min(total_pages, end_page)\n",
    "    \n",
    "    text = \"\"\n",
    "    for page_num in range(start_page - 1, end_page):\n",
    "        page = reader.pages[page_num]\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:\n",
    "            text += page_text + \"\\n\"\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# PDF ë¡œë“œ ì‹¤í–‰\n",
    "pdf_path = PDF_PATH\n",
    "reader = load_pdf(PDF_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2bcc01",
   "metadata": {},
   "source": [
    "---\n",
    "## 4ï¸âƒ£ ëª©ì°¨ íŒŒì‹±\n",
    "\n",
    "ì´ ì…€ì—ì„œëŠ” PDF ëª©ì°¨ë¥¼ íŒŒì‹±í•˜ì—¬ ì±•í„°, íŒŒíŠ¸, ì„œë¸Œì„¹ì…˜ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
    "\n",
    "**ì£¼ìš” í•¨ìˆ˜:**\n",
    "- `create_toc_entry()`: ëª©ì°¨ í•­ëª©ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ìƒì„±\n",
    "- `parse_chapter_line()`, `parse_part_line()`, `parse_subsection_line()`: ê° ìœ í˜•ë³„ ë¼ì¸ íŒŒì‹±\n",
    "- `parse_toc()`: ëª©ì°¨ ì „ì²´ë¥¼ íŒŒì‹±í•˜ì—¬ 3ê°€ì§€ ìœ í˜•ìœ¼ë¡œ ë¶„ë¥˜\n",
    "\n",
    "**ì‹¤í–‰ ê²°ê³¼:**\n",
    "- ì±•í„°, íŒŒíŠ¸, ì„œë¸Œì„¹ì…˜ì˜ ê°œìˆ˜ê°€ ì¶œë ¥ë©ë‹ˆë‹¤.\n",
    "- ì •ê·œì‹ íŒ¨í„´ìœ¼ë¡œ ëª©ì°¨ì˜ êµ¬ì¡°ë¥¼ ì¸ì‹í•˜ì—¬ ê³„ì¸µ êµ¬ì¡°ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61bdba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_toc_entry(entry_type: str, number, title: str, toc_page: int):\n",
    "    \"\"\"TOC í•­ëª© ìƒì„± (ë”•ì…”ë„ˆë¦¬ ë°˜í™˜)\"\"\"\n",
    "    actual_page = toc_page + PAGE_OFFSET\n",
    "    \n",
    "    if entry_type == 'part':\n",
    "        full_title = f\"{number} {title}\"\n",
    "        entry_id = f\"part_{number}\"\n",
    "    elif entry_type == 'subsection':\n",
    "        full_title = f\"{number} {title}\"\n",
    "        entry_id = f\"subsec_{number}\"\n",
    "    else:  # chapter\n",
    "        full_title = f\"{number} {title}\"\n",
    "        entry_id = f\"ch_{number}\"\n",
    "    \n",
    "    return {\n",
    "        'entry_type': entry_type,\n",
    "        'number': number,\n",
    "        'title': title,\n",
    "        'toc_page': toc_page,\n",
    "        'actual_page': actual_page,\n",
    "        'full_title': full_title,\n",
    "        'id': entry_id\n",
    "    }\n",
    "\n",
    "def clean_toc_title(title: str):\n",
    "    \"\"\"ëª©ì°¨ ì œëª© ì •ì œ\"\"\"\n",
    "    title = re.sub(r'\\s*[\\.\\s]{4,}.*$', '', title)\n",
    "    title = re.sub(r'\\s*\\.{3,}.*$', '', title)\n",
    "    title = re.sub(r'\\s*\\.+\\s*$', '', title)\n",
    "    title = re.sub(r'\\s+', ' ', title)\n",
    "    return title.strip()\n",
    "\n",
    "def is_valid_title(title: str):\n",
    "    \"\"\"ì œëª© ìœ íš¨ì„± ê²€ì‚¬\"\"\"\n",
    "    return (bool(title) and \n",
    "            len(title) > 3 and \n",
    "            not title.strip().isdigit() and \n",
    "            title[0].isupper())\n",
    "\n",
    "def parse_chapter_line(line: str):\n",
    "    \"\"\"ì±•í„° ë¼ì¸ íŒŒì‹±\"\"\"\n",
    "    for pattern in MAIN_CHAPTER_PATTERNS:\n",
    "        match = re.match(pattern, line)\n",
    "        if match:\n",
    "            chapter_num = int(match.group(1))\n",
    "            title = clean_toc_title(match.group(2).strip())\n",
    "            page_num = int(match.group(3))\n",
    "            \n",
    "            if is_valid_title(title):\n",
    "                return create_toc_entry('chapter', chapter_num, title, page_num)\n",
    "    return None\n",
    "\n",
    "def parse_part_line(line: str):\n",
    "    \"\"\"íŒŒíŠ¸ ë¼ì¸ íŒŒì‹±\"\"\"\n",
    "    for pattern in PART_PATTERNS:\n",
    "        match = re.match(pattern, line)\n",
    "        if match:\n",
    "            part_num = match.group(1)\n",
    "            title = clean_toc_title(match.group(2).strip())\n",
    "            page_num = int(match.group(3))\n",
    "            \n",
    "            if is_valid_title(title):\n",
    "                return create_toc_entry('part', part_num, title, page_num)\n",
    "    return None\n",
    "\n",
    "def parse_subsection_line(line: str):\n",
    "    \"\"\"ì„œë¸Œì„¹ì…˜ ë¼ì¸ íŒŒì‹± (ì˜ˆ: 2.1, 4.2 ë“±)\"\"\"\n",
    "    for pattern in SUBSECTION_PATTERNS:\n",
    "        match = re.match(pattern, line)\n",
    "        if match:\n",
    "            subsection_num = match.group(1)\n",
    "            title = clean_toc_title(match.group(2).strip())\n",
    "            page_num = int(match.group(3))\n",
    "            \n",
    "            if is_valid_title(title):\n",
    "                return create_toc_entry('subsection', subsection_num, title, page_num)\n",
    "    return None\n",
    "\n",
    "def parse_toc(reader):\n",
    "    \"\"\"ëª©ì°¨ íŒŒì‹± (ì±•í„° + íŒŒíŠ¸ + ì„œë¸Œì„¹ì…˜)\"\"\"\n",
    "    print(f\"ëª©ì°¨ íŒŒì‹± ì¤‘ (í˜ì´ì§€ {TOC_START_PAGE}-{TOC_END_PAGE})...\")\n",
    "    \n",
    "    toc_text = extract_pdf_text(reader, TOC_START_PAGE, TOC_END_PAGE)\n",
    "    if not toc_text:\n",
    "        print(\"ëª©ì°¨ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨\")\n",
    "        return [], [], []\n",
    "    \n",
    "    chapters = []\n",
    "    parts = []\n",
    "    subsections = []\n",
    "    \n",
    "    for line in toc_text.split('\\n'):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        \n",
    "        # íŒŒíŠ¸ íŒŒì‹± ìš°ì„  ì‹œë„\n",
    "        part_entry = parse_part_line(line)\n",
    "        if part_entry:\n",
    "            parts.append(part_entry)\n",
    "            continue\n",
    "        \n",
    "        # ì„œë¸Œì„¹ì…˜ íŒŒì‹± ì‹œë„\n",
    "        subsection_entry = parse_subsection_line(line)\n",
    "        if subsection_entry:\n",
    "            subsections.append(subsection_entry)\n",
    "            continue\n",
    "        \n",
    "        # ì±•í„° íŒŒì‹±\n",
    "        chapter_entry = parse_chapter_line(line)\n",
    "        if chapter_entry:\n",
    "            chapters.append(chapter_entry)\n",
    "    \n",
    "    # í˜ì´ì§€ ìˆœì„œë¡œ ì •ë ¬\n",
    "    chapters.sort(key=lambda x: x['toc_page'])\n",
    "    parts.sort(key=lambda x: x['toc_page'])\n",
    "    subsections.sort(key=lambda x: x['toc_page'])\n",
    "    \n",
    "    print(f\"ëª©ì°¨ íŒŒì‹± ì™„ë£Œ: {len(chapters)}ê°œ ì±•í„°, {len(parts)}ê°œ íŒŒíŠ¸, {len(subsections)}ê°œ ì„œë¸Œì„¹ì…˜\")\n",
    "    return chapters, parts, subsections\n",
    "\n",
    "# ëª©ì°¨ íŒŒì‹± ì‹¤í–‰\n",
    "chapters, parts, subsections = parse_toc(reader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cee5e56",
   "metadata": {},
   "source": [
    "---\n",
    "## 5ï¸âƒ£ ì±•í„° ë²”ìœ„ ê³„ì‚°\n",
    "\n",
    "ì´ ì…€ì—ì„œëŠ” ê° ì±•í„°ê°€ ì°¨ì§€í•˜ëŠ” í˜ì´ì§€ ë²”ìœ„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "\n",
    "**ì£¼ìš” í•¨ìˆ˜:**\n",
    "- `find_part_for_chapter()`: ì±•í„°ê°€ ì†í•œ íŒŒíŠ¸ë¥¼ ì°¾ê¸°\n",
    "- `calculate_chapter_ranges()`: ì±•í„°ë³„ ì‹œì‘/ì¢…ë£Œ í˜ì´ì§€ë¥¼ ê³„ì‚°\n",
    "\n",
    "**ì‹¤í–‰ ê²°ê³¼:**\n",
    "- ê° ì±•í„°ì˜ í˜ì´ì§€ ë²”ìœ„ê°€ ê³„ì‚°ë©ë‹ˆë‹¤.\n",
    "- ë‹¤ìŒ ì±•í„° ì‹œì‘ ì „ê¹Œì§€ê°€ í˜„ì¬ ì±•í„°ì˜ ë§ˆì§€ë§‰ í˜ì´ì§€ì…ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c0a612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_part_for_chapter(chapter, parts):\n",
    "    \"\"\"ì±•í„°ê°€ ì†í•œ íŒŒíŠ¸ ì°¾ê¸°\"\"\"\n",
    "    current_part = None\n",
    "    for part in parts:\n",
    "        if part['actual_page'] <= chapter['actual_page']:\n",
    "            current_part = part\n",
    "        else:\n",
    "            break\n",
    "    return current_part\n",
    "\n",
    "def calculate_chapter_ranges(reader, chapters, parts):\n",
    "    \"\"\"ì±•í„°ë³„ í˜ì´ì§€ ë²”ìœ„ ê³„ì‚°\"\"\"\n",
    "    print(\"ì±•í„°ë³„ í˜ì´ì§€ ë²”ìœ„ ê³„ì‚° ì¤‘...\")\n",
    "    \n",
    "    if not chapters:\n",
    "        print(\"ë©”ì¸ ì±•í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return []\n",
    "    \n",
    "    chapter_ranges = []\n",
    "    total_pages = len(reader.pages)\n",
    "    \n",
    "    for i, chapter in enumerate(chapters):\n",
    "        start_page = chapter['actual_page']\n",
    "        \n",
    "        # ë‹¤ìŒ ì±•í„°ê°€ ìˆìœ¼ë©´ ê·¸ ì „ í˜ì´ì§€ê¹Œì§€, ì—†ìœ¼ë©´ PDF ëê¹Œì§€\n",
    "        if i + 1 < len(chapters):\n",
    "            end_page = chapters[i + 1]['actual_page'] - 1\n",
    "        else:\n",
    "            end_page = total_pages\n",
    "        \n",
    "        # í•´ë‹¹ ì±•í„°ê°€ ì†í•œ íŒŒíŠ¸ ì°¾ê¸°\n",
    "        part_info = find_part_for_chapter(chapter, parts)\n",
    "        \n",
    "        # í˜ì´ì§€ ë²”ìœ„ ìœ íš¨ì„± ê²€ì‚¬\n",
    "        if start_page <= end_page:\n",
    "            chapter_range = {\n",
    "                'toc_entry': chapter,\n",
    "                'part_info': part_info,\n",
    "                'start_page': start_page,\n",
    "                'end_page': end_page,\n",
    "                'page_count': end_page - start_page + 1\n",
    "            }\n",
    "            chapter_ranges.append(chapter_range)\n",
    "    \n",
    "    print(f\"ì±•í„° ë²”ìœ„ ê³„ì‚° ì™„ë£Œ: {len(chapter_ranges)}ê°œ ì±•í„°\")\n",
    "    return chapter_ranges\n",
    "\n",
    "# ì±•í„° ë²”ìœ„ ê³„ì‚°\n",
    "chapter_ranges = calculate_chapter_ranges(reader, chapters, parts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418f526e",
   "metadata": {},
   "source": [
    "---\n",
    "## 6ï¸âƒ£ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì„œë¸Œì„¹ì…˜ ë¶„í• \n",
    "\n",
    "ì´ ì…€ì—ì„œëŠ” ê° ì±•í„°ì˜ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  ì„œë¸Œì„¹ì…˜ìœ¼ë¡œ ë¶„í• í•©ë‹ˆë‹¤.\n",
    "\n",
    "**ì£¼ìš” í•¨ìˆ˜:**\n",
    "- `clean_text()`: PDF í—¤ë”/í‘¸í„°, í˜ì´ì§€ ë²ˆí˜¸ ì œê±° ë° ê³µë°± ì •ë¦¬\n",
    "- `find_chapter_subsections()`: ì±•í„°ì— ì†í•œ ì„œë¸Œì„¹ì…˜ë“¤ ì°¾ê¸°\n",
    "- `split_text_by_subsections()`: í…ìŠ¤íŠ¸ë¥¼ ì„œë¸Œì„¹ì…˜ë³„ë¡œ ë¶„í• \n",
    "- `extract_texts()`: ëª¨ë“  ì±•í„°ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì„œë¸Œì„¹ì…˜ ë¶„í• \n",
    "\n",
    "**ì‹¤í–‰ ê²°ê³¼:**\n",
    "- ê° ì±•í„°ì˜ í…ìŠ¤íŠ¸ê°€ ì¶”ì¶œë˜ê³  ì„œë¸Œì„¹ì…˜ë³„ë¡œ ë¶„í• ë©ë‹ˆë‹¤.\n",
    "- í—¤ë”/í‘¸í„° ë“± ë¶ˆí•„ìš”í•œ ìš”ì†Œê°€ ì œê±°ëœ ê¹¨ë—í•œ í…ìŠ¤íŠ¸ë¥¼ ì–»ìŠµë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0b0db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str):\n",
    "    \"\"\"í…ìŠ¤íŠ¸ ì •ì œ ë° í—¤ë”/í‘¸í„° ì œê±°\"\"\"\n",
    "    # PDF í—¤ë”/í‘¸í„° ì œê±°\n",
    "    header_pattern = r'c/circle\\s*copyrt\\s*\\d+,?\\s*A\\s+RPACI\\s*-?\\s*D\\s*USSEAU\\s*THREE\\s+EASY\\s+PIECES'\n",
    "    text = re.sub(header_pattern, '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    footer_pattern = r'OPERATING\\s+SYSTEMS\\s+\\[VERSION\\s+[\\d.]+\\]\\s+WWW\\s*\\.\\s*OSTEP\\s*\\.\\s*ORG.*$'\n",
    "    text = re.sub(footer_pattern, '', text, flags=re.IGNORECASE | re.MULTILINE)\n",
    "    \n",
    "    # í˜ì´ì§€ ë²ˆí˜¸ íŒ¨í„´ ì œê±°\n",
    "    text = re.sub(r'^\\s*\\d+\\s*$', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # ê³µë°± ì •ë¦¬\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def find_chapter_subsections(chapter, subsections):\n",
    "    \"\"\"ì±•í„°ì— ì†í•œ ì„œë¸Œì„¹ì…˜ë“¤ ì°¾ê¸°\"\"\"\n",
    "    chapter_num = str(chapter['number'])\n",
    "    chapter_subsections = []\n",
    "    \n",
    "    for subsection in subsections:\n",
    "        # ì„œë¸Œì„¹ì…˜ ë²ˆí˜¸ê°€ í•´ë‹¹ ì±•í„°ë¡œ ì‹œì‘í•˜ëŠ”ì§€ í™•ì¸ (ì˜ˆ: 2.1, 2.2ëŠ” ì±•í„° 2ì— ì†í•¨)\n",
    "        subsection_num = str(subsection['number'])\n",
    "        if subsection_num.startswith(f\"{chapter_num}.\"):\n",
    "            chapter_subsections.append(subsection)\n",
    "    \n",
    "    # ë²ˆí˜¸ ìˆœìœ¼ë¡œ ì •ë ¬\n",
    "    chapter_subsections.sort(key=lambda x: float(x['number']))\n",
    "    return chapter_subsections\n",
    "\n",
    "def build_flexible_title_pattern(title: str):\n",
    "    \"\"\"ì œëª©ì„ ì•ˆì „í•œ ì •ê·œì‹ìœ¼ë¡œ ë³€í™˜\"\"\"\n",
    "    parts = []\n",
    "    for ch in title:\n",
    "        if ch.isspace():\n",
    "            parts.append(r\"\\s+\")\n",
    "        elif ch == '(':\n",
    "            parts.append(r\"\\s*\\(\\s*\")\n",
    "        elif ch == ')':\n",
    "            parts.append(r\"\\s*\\)\\s*\")\n",
    "        else:\n",
    "            parts.append(re.escape(ch))\n",
    "    return ''.join(parts)\n",
    "\n",
    "def split_text_by_subsections(chapter_text: str, chapter, chapter_subsections):\n",
    "    \"\"\"ì±•í„° í…ìŠ¤íŠ¸ë¥¼ ì„œë¸Œì„¹ì…˜ë³„ë¡œ ë¶„í• \"\"\"\n",
    "    if not chapter_subsections:\n",
    "        return []\n",
    "    \n",
    "    subsection_data_list = []\n",
    "    \n",
    "    split_points = []\n",
    "    \n",
    "    for subsection in chapter_subsections:\n",
    "        section_title = f\"{subsection['number']} {subsection['title']}\"\n",
    "        \n",
    "        patterns_to_try = [\n",
    "            re.escape(section_title),\n",
    "            f\"{re.escape(str(subsection['number']))}\\\\s+{build_flexible_title_pattern(subsection['title'])}\"\n",
    "        ]\n",
    "\n",
    "        # \"The\" ì‹œì‘í•˜ëŠ” ì œëª©ì˜ íŠ¹ìˆ˜ ì²˜ë¦¬\n",
    "        lower_title = subsection['title'].lower()\n",
    "        if lower_title.startswith(\"the \"):\n",
    "            after_the = subsection['title'][4:]\n",
    "            patterns_to_try.append(\n",
    "                f\"{re.escape(str(subsection['number']))}\\\\s+the\\\\s*{build_flexible_title_pattern(after_the)}\"\n",
    "            )\n",
    "        elif lower_title.startswith(\"the\"):\n",
    "            after_the = subsection['title'][3:]\n",
    "            patterns_to_try.append(\n",
    "                f\"{re.escape(str(subsection['number']))}\\\\s+the\\\\s*{build_flexible_title_pattern(after_the)}\"\n",
    "            )\n",
    "        \n",
    "        found = False\n",
    "        for pattern in patterns_to_try:\n",
    "            matches = list(re.finditer(pattern, chapter_text, re.IGNORECASE | re.MULTILINE))\n",
    "            if matches:\n",
    "                # ì²« ë²ˆì§¸ ë§¤ì¹˜ ì‚¬ìš©\n",
    "                match = matches[0]\n",
    "                split_points.append({\n",
    "                    'subsection': subsection,\n",
    "                    'start_pos': match.start(),\n",
    "                    'end_pos': match.end(),\n",
    "                    'title_match': match.group().strip()\n",
    "                })\n",
    "                found = True\n",
    "                break\n",
    "        \n",
    "        if not found:\n",
    "            print(f\"  ì„œë¸Œì„¹ì…˜ '{subsection['number']} {subsection['title']}' ì œëª©ì„ í…ìŠ¤íŠ¸ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    # ìœ„ì¹˜ë³„ë¡œ ì •ë ¬\n",
    "    split_points.sort(key=lambda x: x['start_pos'])\n",
    "    \n",
    "    # í…ìŠ¤íŠ¸ ë¶„í• \n",
    "    for i, split_point in enumerate(split_points):\n",
    "        start_pos = split_point['start_pos']\n",
    "        \n",
    "        # ë‹¤ìŒ ì„œë¸Œì„¹ì…˜ì˜ ì‹œì‘ ìœ„ì¹˜ ë˜ëŠ” í…ìŠ¤íŠ¸ ë\n",
    "        if i + 1 < len(split_points):\n",
    "            end_pos = split_points[i + 1]['start_pos']\n",
    "        else:\n",
    "            end_pos = len(chapter_text)\n",
    "        \n",
    "        # ì„œë¸Œì„¹ì…˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "        subsection_text = chapter_text[start_pos:end_pos].strip()\n",
    "        \n",
    "        if subsection_text:\n",
    "            subsection_data = {\n",
    "                'toc_entry': split_point['subsection'],\n",
    "                'parent_chapter': chapter,\n",
    "                'text': clean_text(subsection_text)\n",
    "            }\n",
    "            subsection_data_list.append(subsection_data)\n",
    "    \n",
    "    return subsection_data_list\n",
    "\n",
    "def extract_texts(reader, chapter_ranges, subsections):\n",
    "    \"\"\"ì±•í„°ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì„œë¸Œì„¹ì…˜ ë¶„í• \"\"\"\n",
    "    print(f\"ì±•í„°ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì„œë¸Œì„¹ì…˜ ë¶„í•  ì¤‘...\")\n",
    "    for chapter_range in chapter_ranges:\n",
    "        chapter = chapter_range['toc_entry']\n",
    "        \n",
    "        # í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "        text = extract_pdf_text(reader, chapter_range['start_page'], chapter_range['end_page'])\n",
    "        chapter_range['text'] = clean_text(text)\n",
    "        \n",
    "        # í•´ë‹¹ ì±•í„°ì˜ ì„œë¸Œì„¹ì…˜ë“¤ ì°¾ê¸°\n",
    "        chapter_subsections = find_chapter_subsections(chapter, subsections)\n",
    "        \n",
    "        if chapter_subsections:\n",
    "            # ì„œë¸Œì„¹ì…˜ë³„ë¡œ í…ìŠ¤íŠ¸ ë¶„í• \n",
    "            subsection_data_list = split_text_by_subsections(chapter_range['text'], chapter, chapter_subsections)\n",
    "            chapter_range['subsections'] = subsection_data_list\n",
    "        else:\n",
    "            chapter_range['subsections'] = []\n",
    "    \n",
    "    print(f\"ì±•í„°ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì„œë¸Œì„¹ì…˜ ë¶„í•  ì™„ë£Œ: {len(chapter_ranges)}ê°œ ì±•í„°\")\n",
    "    return chapter_ranges\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì„œë¸Œì„¹ì…˜ ë¶„í•  ì‹¤í–‰\n",
    "chapter_ranges = extract_texts(reader, chapter_ranges, subsections)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9b194f",
   "metadata": {},
   "source": [
    "---\n",
    "## 7ï¸âƒ£ ì²­í¬ ìƒì„± ë° JSON ì €ì¥\n",
    "\n",
    "ì´ ì…€ì—ì„œëŠ” í…ìŠ¤íŠ¸ë¥¼ í† í° ê¸°ë°˜ìœ¼ë¡œ ì²­í‚¹í•˜ê³  JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "**ì£¼ìš” í•¨ìˆ˜:**\n",
    "- `estimate_tokens_length()`: tiktoken ë˜ëŠ” ê·¼ì‚¬ì¹˜ë¡œ í† í° ìˆ˜ ì¶”ì •\n",
    "- `split_text_into_sentences()`: ì •ê·œì‹ìœ¼ë¡œ ë¬¸ì¥ ë¶„ë¦¬\n",
    "- `chunk_sentences_hybrid()`: ë¬¸ì¥ ê²½ê³„ë¥¼ ìœ ì§€í•˜ë©° í† í° ì œí•œìœ¼ë¡œ ì²­í‚¹\n",
    "- `build_all_chunk_records()`: ëª¨ë“  ì²­í¬ë¥¼ ë ˆì½”ë“œë¡œ ìƒì„±\n",
    "- `write_json()`: JSON íŒŒì¼ë¡œ ì €ì¥\n",
    "\n",
    "**ì‹¤í–‰ ê²°ê³¼:**\n",
    "- ë¬¸ì¥ ê²½ê³„ë¥¼ ìœ ì§€í•˜ë©´ì„œ í† í° ì œí•œ(400í† í°, 20% ì˜¤ë²„ë©)ì— ë§ì¶° ì²­í‚¹ë©ë‹ˆë‹¤.\n",
    "- JSON íŒŒì¼ë¡œ ì €ì¥ë˜ì–´ RAG ì‹œìŠ¤í…œì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2b6d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_output_dir(output_dir: str):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def build_output_path(output_dir: str, max_tokens: int, overlap_tokens: int):\n",
    "    overlap_pct = int(round((overlap_tokens / max_tokens) * 100)) if max_tokens > 0 else 0\n",
    "    filename = f\"{DOC_ID}_tok{max_tokens}_ov{overlap_pct}.json\"\n",
    "    return str(Path(output_dir) / filename)\n",
    "\n",
    "def try_import_tiktoken():\n",
    "    try:\n",
    "        import tiktoken\n",
    "        return tiktoken\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def estimate_tokens_length(text: str):\n",
    "    \"\"\"í† í° ê¸¸ì´ ì¶”ì • (tiktoken ìš°ì„ , ì—†ìœ¼ë©´ ê·¼ì‚¬ì¹˜)\"\"\"\n",
    "    tiktoken = try_import_tiktoken()\n",
    "    if tiktoken is not None:\n",
    "        try:\n",
    "            enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        except Exception:\n",
    "            enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        return len(enc.encode(text))\n",
    "    return int(len(text.split()) * 1.3)\n",
    "\n",
    "def split_text_into_sentences(text: str):\n",
    "    \"\"\"ì •ê·œì‹ ê¸°ë°˜ ë¬¸ì¥ ë¶„ë¦¬\"\"\"\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    sentences = re.split(SENTENCE_SPLIT_REGEX, text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    return sentences\n",
    "\n",
    "def chunk_sentences_hybrid(sentences, max_tokens: int, overlap_tokens: int):\n",
    "    \"\"\"ë¬¸ì¥ ê²½ê³„ë¥¼ ìœ ì§€í•˜ë©° í† í° ìƒí•œìœ¼ë¡œ ì²­í‚¹\"\"\"\n",
    "    chunks = []\n",
    "    if not sentences:\n",
    "        return chunks\n",
    "\n",
    "    current = []\n",
    "    current_tokens = 0\n",
    "    for sent in sentences:\n",
    "        sent_tokens = estimate_tokens_length(sent)\n",
    "        \n",
    "        if sent_tokens > max_tokens:\n",
    "            if current:\n",
    "                chunks.append(\" \".join(current).strip())\n",
    "                current = []\n",
    "                current_tokens = 0\n",
    "            chunks.append(sent.strip())\n",
    "            continue\n",
    "\n",
    "        if current_tokens + sent_tokens <= max_tokens:\n",
    "            current.append(sent)\n",
    "            current_tokens += sent_tokens\n",
    "        else:\n",
    "            if current:\n",
    "                chunks.append(\" \".join(current).strip())\n",
    "            \n",
    "            if overlap_tokens > 0:\n",
    "                overlap_bucket = []\n",
    "                overlap_count = 0\n",
    "                for prev_sent in reversed(current):\n",
    "                    t = estimate_tokens_length(prev_sent)\n",
    "                    if overlap_count + t > overlap_tokens:\n",
    "                        break\n",
    "                    overlap_bucket.append(prev_sent)\n",
    "                    overlap_count += t\n",
    "                overlap_bucket.reverse()\n",
    "                current = overlap_bucket + [sent]\n",
    "                current_tokens = sum(estimate_tokens_length(s) for s in current)\n",
    "            else:\n",
    "                current = [sent]\n",
    "                current_tokens = sent_tokens\n",
    "\n",
    "    if current:\n",
    "        chunks.append(\" \".join(current).strip())\n",
    "    return chunks\n",
    "\n",
    "def make_chunk_id(chapter_id: str, subsec_id, chunk_index: int):\n",
    "    base = f\"{chapter_id}\"\n",
    "    if subsec_id:\n",
    "        base += f\"__{subsec_id}\"\n",
    "    return f\"{base}__{chunk_index:04d}\"\n",
    "\n",
    "def find_part_for_page(page: int, part_lookup_by_page):\n",
    "\tcurrent = None\n",
    "\tfor start_page, part in part_lookup_by_page:\n",
    "\t\tif start_page <= page:\n",
    "\t\t\tcurrent = part\n",
    "\t\telse:\n",
    "\t\t\tbreak\n",
    "\treturn current\n",
    "\n",
    "def build_all_chunk_records(pdf_path: str, parts, chapter_ranges, max_tokens: int, overlap_tokens: int):\n",
    "    part_lookup_by_page = []\n",
    "    for p in parts:\n",
    "        part_lookup_by_page.append((p['actual_page'], p))\n",
    "    part_lookup_by_page.sort(key=lambda x: x[0])\n",
    "\n",
    "    records = []\n",
    "\n",
    "    for cr in chapter_ranges:\n",
    "        chapter = cr['toc_entry']\n",
    "        chapter_id = chapter['id']\n",
    "        chapter_title = chapter['title']\n",
    "        part_info = cr.get('part_info')\n",
    "        if part_info is None:\n",
    "            part_info = find_part_for_page(cr['start_page'], part_lookup_by_page)\n",
    "\n",
    "        # ì„œë¸Œì„¹ì…˜ì´ ìˆìœ¼ë©´ ì„œë¸Œì„¹ì…˜ ê¸°ì¤€, ì—†ìœ¼ë©´ ì±•í„° ì „ì²´ í…ìŠ¤íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ì²­í‚¹\n",
    "        if cr.get('subsections'):\n",
    "            for sub in cr['subsections']:\n",
    "                subsec = sub\n",
    "                sentences = split_text_into_sentences(subsec['text'])\n",
    "                chunks = chunk_sentences_hybrid(sentences, max_tokens, overlap_tokens)\n",
    "                for idx, chunk_text in enumerate(chunks):\n",
    "                    records.append({\n",
    "                        'chunk_id': make_chunk_id(chapter_id, subsec['toc_entry']['id'], idx),\n",
    "                        'chapter_id': chapter_id,\n",
    "                        'chapter_title': chapter_title,\n",
    "                        'subsection_id': subsec['toc_entry']['id'],\n",
    "                        'subsection_title': f\"{subsec['toc_entry']['number']} {subsec['toc_entry']['title']}\",\n",
    "                        'text': chunk_text,\n",
    "                    })\n",
    "        else:\n",
    "            sentences = split_text_into_sentences(cr['text'])\n",
    "            chunks = chunk_sentences_hybrid(sentences, max_tokens, overlap_tokens)\n",
    "            for idx, chunk_text in enumerate(chunks):\n",
    "                records.append({\n",
    "                    'chunk_id': make_chunk_id(chapter_id, None, idx),\n",
    "                    'chapter_id': chapter_id,\n",
    "                    'chapter_title': chapter_title,\n",
    "                    'subsection_id': None,\n",
    "                    'subsection_title': None,\n",
    "                    'text': chunk_text,\n",
    "                })\n",
    "\n",
    "    return records\n",
    "\n",
    "def write_json(path: str, records):\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# ì²­í¬ ìƒì„± ë° ì €ì¥\n",
    "print(\"\\n5ë‹¨ê³„: ì²­í¬ ìƒì„± ë° ì €ì¥(JSON)\")\n",
    "print(\"-\" * 40)\n",
    "ensure_output_dir(OUTPUT_DIR)\n",
    "output_path = build_output_path(OUTPUT_DIR, CHUNK_MAX_TOKENS, CHUNK_OVERLAP_TOKENS)\n",
    "all_chunk_records = build_all_chunk_records(\n",
    "\tpdf_path=pdf_path,\n",
    "\tparts=parts,\n",
    "\tchapter_ranges=chapter_ranges,\n",
    "\tmax_tokens=CHUNK_MAX_TOKENS,\n",
    "\toverlap_tokens=CHUNK_OVERLAP_TOKENS,\n",
    ")\n",
    "write_json(output_path, all_chunk_records)\n",
    "print(f\"ì²­í¬ ì €ì¥ ì™„ë£Œ: {output_path} ({len(all_chunk_records)} chunks)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b9d4bf",
   "metadata": {},
   "source": [
    "---\n",
    "## 8ï¸âƒ£ ê²°ê³¼ ìš”ì•½ ì¶œë ¥\n",
    "\n",
    "ì´ ì…€ì—ì„œëŠ” ì „ì²˜ë¦¬ëœ ë°ì´í„°ì˜ ìš”ì•½ ì •ë³´ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "**ì£¼ìš” ë‚´ìš©:**\n",
    "- ì±•í„°ë³„ í˜ì´ì§€ ë²”ìœ„, ë¬¸ì ìˆ˜, ì„œë¸Œì„¹ì…˜ ê°œìˆ˜\n",
    "- ì„œë¸Œì„¹ì…˜ë³„ ë¬¸ì ìˆ˜\n",
    "\n",
    "**ì‹¤í–‰ ê²°ê³¼:**\n",
    "- ê° ì±•í„°ì™€ ì„œë¸Œì„¹ì…˜ì˜ ìƒì„¸ ì •ë³´ê°€ ì¶œë ¥ë©ë‹ˆë‹¤.\n",
    "- ì „ì²˜ë¦¬ í’ˆì§ˆì„ í™•ì¸í•˜ê³  ê°œì„ ì ì„ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104af0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ê³¼ ìš”ì•½\n",
    "total_subsections = sum(len(cr.get('subsections', [])) for cr in chapter_ranges)\n",
    "\n",
    "print(\"\\nChapter information:\")\n",
    "for info in chapter_ranges:\n",
    "    part_str = f\"Part {info['part_info']['number']}\" if info['part_info'] else 'Unclassified'\n",
    "    print(f\"  â€¢ {info['toc_entry']['full_title']} ({part_str}): {info['start_page']}-{info['end_page']} \"\n",
    "        f\"({info['page_count']}p, {len(info['text']):,} chars, {len(info.get('subsections', []))} subsections)\")\n",
    "    \n",
    "    # ì„œë¸Œì„¹ì…˜ ì •ë³´ ì¶œë ¥\n",
    "    if info.get('subsections'):\n",
    "        for subsection in info['subsections']:\n",
    "            print(f\"    â”œâ”€ {subsection['toc_entry']['number']} {subsection['toc_entry']['title']} ({len(subsection['text']):,} chars)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
