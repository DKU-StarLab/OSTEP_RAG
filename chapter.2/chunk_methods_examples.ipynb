{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "colab_intro",
   "metadata": {},
   "source": [
    "\n",
    "# Chapter 2: í…ìŠ¤íŠ¸ ì²­í‚¹ ê¸°ë²• ì‹¤ìŠµ (ì˜ˆì œ)\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ ë‹¤ì–‘í•œ ì²­í‚¹ ì „ëžµì„ ì˜ˆì œë¡œ ì‹¤ìŠµí•˜ë©° ìž¥ë‹¨ì ê³¼ ì ìš© ë§¥ë½ì„ ë¹ ë¥´ê²Œ íŒŒì•…í•  ìˆ˜ ìžˆë„ë¡ êµ¬ì„±ë˜ì–´ ìžˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "## ðŸ“š í•™ìŠµ ëª©í‘œ\n",  
    "- ëŒ€í‘œì  ì²­í‚¹ ì „ëžµ(ê³ ì • ê¸¸ì´/ìŠ¬ë¼ì´ë”©/êµ¬ì¡°/ê³„ì¸µ)ì˜ ê°œë…ê³¼ ì°¨ì´ ì´í•´\n",
    "- ê°„ë‹¨í•œ ì˜ˆë¬¸ìœ¼ë¡œ ê° ê¸°ë²•ì„ ì§ì ‘ ì‹¤í–‰í•´ ê²°ê³¼ ë¹„êµ\n",
    "- ê³¼/ì†Œì²­í‚¹ í˜„ìƒê³¼ ë¬¸ë§¥ ë³´ì¡´ì˜ íŠ¸ë ˆì´ë“œì˜¤í”„ ì²´ê°\n",
    "\n",
    "## ðŸ“‹ ì‹¤ìŠµ êµ¬ì„±\n",  
    "- 1ï¸âƒ£ í™˜ê²½ ì„¤ì •: Colab ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸ ë° ê²½ë¡œ ì„¤ì •\n",
    "- 2ï¸âƒ£ ê³ ì • ê¸¸ì´ ì²­í‚¹: ë¬¸ìž¥/ë‹¨ì–´ ë‹¨ìœ„ ê³ ì • í¬ê¸° ë¶„í• \n",
    "- 3ï¸âƒ£ ìŠ¬ë¼ì´ë”© ìœˆë„ìš°: ê²¹ì¹¨ ë¹„ìœ¨ì„ ì¡°ì ˆí•˜ë©° ì¤‘ë³µ ë¬¸ë§¥ ìœ ì§€\n",
    "- 4ï¸âƒ£ êµ¬ì¡° ê¸°ë°˜: ë§ˆí¬ë‹¤ìš´ í—¤ë”/ì½”ë“œë¸”ë¡ ê²½ê³„ë¥¼ í™œìš©í•œ ë¶„í• \n",
    "- 5ï¸âƒ£ ê³„ì¸µ ê¸°ë°˜: ì„¹ì…˜ ì œëª©ì„ í—¤ë”ë¡œ í¬í•¨í•´ ìƒí•˜ìœ„ ë§¥ë½ ìœ ì§€\n",
    "\n",
    "> âš ï¸ ì‹¤ìŠµ ì…€ ì‹¤í–‰ ì „, í™˜ê²½ ì„¤ì • ì…€(1ï¸âƒ£)ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b9c87b",
   "metadata": {},
   "source": [
    "---\n",
    "## 1ï¸âƒ£ Google Colab í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colab_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Google Colab í™˜ê²½ ì„¤ì •\n",
    "# ========================================\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Google Drive ë§ˆìš´íŠ¸\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# ê²½ë¡œ ì„¤ì •\n",
    "BASE_DIR = \"/content/drive/MyDrive/ostep_rag\"\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "\n",
    "print(\"âœ… Colab í™˜ê²½ ì„¤ì • ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69d9401",
   "metadata": {},
   "source": [
    "\n",
    "# í…ìŠ¤íŠ¸ ì²­í‚¹ ì‹¤ìŠµ ë…¸íŠ¸ë¶\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ **ê³ ì • ê¸¸ì´**, **ìŠ¬ë¼ì´ë”© ìœˆë„ìš°**, **ì˜ë¯¸ ê¸°ë°˜**, **êµ¬ì¡° ê¸°ë°˜**, **ê³„ì¸µ êµ¬ì¡°** ë“± 5ê°€ì§€ ì²­í‚¹ ì „ëžµì„ ê°ê°ì˜ ì…€ë¡œ ì‹¤ìŠµí•  ìˆ˜ ìžˆë„ë¡ êµ¬ì„±ë˜ì–´ ìžˆìŠµë‹ˆë‹¤.  \n",
    "ê° ì„¹ì…˜ì—ëŠ” (1) ì„¤ëª…, (2) ì˜ˆë¬¸, (3) ì‹¤í–‰ ì½”ë“œê°€ í¬í•¨ë˜ì–´ ìžˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "> íŒ: ê° ì…€ì„ ìœ„ì—ì„œë¶€í„° ìˆœì„œëŒ€ë¡œ ì‹¤í–‰í•˜ì„¸ìš”. ê²°ê³¼ëŠ” `chunks` ë¦¬ìŠ¤íŠ¸ í˜¹ì€ í‘œ í˜•íƒœë¡œ ì¶œë ¥ë©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca42f7a",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## 2ï¸âƒ£ ê³ ì • ê¸¸ì´ ì²­í‚¹ (Fixed-length)\n",
    "\n",
    "**ê°œë…**: ë¬¸ì„œë¥¼ ì¼ì •í•œ ë‹¨ìœ„(ë¬¸ìž¥/ë‹¨ì–´/ë¬¸ìž)ë¡œ **ê· ë“±í•˜ê²Œ** ë‚˜ëˆ•ë‹ˆë‹¤.  \n",
    "**ìž¥ì **: êµ¬í˜„ì´ ë‹¨ìˆœí•˜ê³  ë¹ ë¥´ë©° ë³‘ë ¬ ì²˜ë¦¬ì— ìœ ë¦¬.  \n",
    "**ë‹¨ì **: ë¬¸ë§¥ì´ ëŠê¸°ê±°ë‚˜ ì§ˆì˜ì™€ ë¬´ê´€í•œ ì •ë³´ê°€ í¬í•¨ë  ìˆ˜ ìžˆìŒ.\n",
    "\n",
    "ì•„ëž˜ ì˜ˆë¬¸ì€ ë¬¸ìž¥ 3ê°œì”© ê³ ì • ê¸¸ì´ë¡œ ë¶„í• í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7297a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n",
    "\n",
    "def simple_sent_split(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into sentences using simple rules.\n",
    "    - Split on periods, question marks, exclamation marks\n",
    "    - Split on line breaks\n",
    "    \"\"\"\n",
    "    # First split by line breaks\n",
    "    parts = re.split(r\"\\n+\", text.strip())\n",
    "    sentences = []\n",
    "    \n",
    "    for part in parts:\n",
    "        # Split on punctuation marks\n",
    "        split_sentences = re.split(r\"(?<=[\\.!\\?])\\s+\", part.strip())\n",
    "        # Filter out empty strings\n",
    "        split_sentences = [s for s in split_sentences if s]\n",
    "        sentences.extend(split_sentences)\n",
    "    \n",
    "    # Clean up: remove extra whitespace\n",
    "    return [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "def chunk_by_fixed_length(text: str, unit: str = \"sentences\", size: int = 3) -> List[str]:\n",
    "    \"\"\"\n",
    "    Fixed-length chunking: Split text into equal-sized chunks.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to chunk\n",
    "        unit: \"sentences\", \"words\", or \"chars\"\n",
    "        size: Number of units per chunk\n",
    "    \n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    \"\"\"\n",
    "    # Step 1: Split text into basic units\n",
    "    if unit == \"sentences\":\n",
    "        items = simple_sent_split(text)\n",
    "    elif unit == \"words\":\n",
    "        # Split on whitespace to get words\n",
    "        items = re.findall(r\"\\S+\", text)\n",
    "    elif unit == \"chars\":\n",
    "        # Split into individual characters\n",
    "        items = list(text)\n",
    "    else:\n",
    "        raise ValueError(\"unit must be one of: sentences, words, chars\")\n",
    "\n",
    "    # Step 2: Group items into chunks of specified size\n",
    "    chunks = []\n",
    "    for i in range(0, len(items), size):\n",
    "        # Get a slice of items for this chunk\n",
    "        chunk_items = items[i:i+size]\n",
    "        \n",
    "        # Join items back into text\n",
    "        if unit == \"sentences\":\n",
    "            chunks.append(\" \".join(chunk_items))\n",
    "        elif unit == \"words\":\n",
    "            chunks.append(\" \".join(chunk_items))\n",
    "        else:  # chars\n",
    "            chunks.append(\"\".join(chunk_items))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def show_chunks(chunks):\n",
    "    \"\"\"Display chunks in a numbered format\"\"\"\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        print(f\"[{i}] {chunk}\\n\")\n",
    "\n",
    "# Example: Let's chunk a simple text about machine learning\n",
    "text = \"\"\"\n",
    "Machine learning is a subset of artificial intelligence. It focuses on algorithms that can learn from data. The goal is to make predictions or decisions without being explicitly programmed.\n",
    "There are three main types of machine learning. Supervised learning uses labeled training data. Unsupervised learning finds patterns in unlabeled data. Reinforcement learning learns through trial and error.\n",
    "Deep learning is a subset of machine learning. It uses neural networks with multiple layers. These networks can learn complex patterns in data. They have been very successful in image recognition and natural language processing.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== Fixed-length chunking by sentences (3 sentences per chunk) ===\")\n",
    "chunks = chunk_by_fixed_length(text, unit=\"sentences\", size=3)\n",
    "show_chunks(chunks)\n",
    "\n",
    "print(\"=== Fixed-length chunking by words (15 words per chunk) ===\")\n",
    "chunks_words = chunk_by_fixed_length(text, unit=\"words\", size=15)\n",
    "show_chunks(chunks_words[:3])  # Show first 3 chunks only\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23b953d",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034660fd",
   "metadata": {},
   "source": [
    "\n",
    "## 2) ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì²­í‚¹ (Sliding Window)\n",
    "\n",
    "**ê°œë…**: ì¼ì • ê¸¸ì´ì˜ ìœˆë„ìš°ë¥¼ **ê²¹ì¹˜ê²Œ** ì´ë™ì‹œí‚¤ë©° ë¶„í• í•©ë‹ˆë‹¤.  \n",
    "**ìž¥ì **: ì¸ì ‘ ì²­í¬ ê°„ ë¬¸ë§¥ì´ ìœ ì§€ë˜ì–´ **ì˜ë¯¸ ë‹¨ì ˆ ìµœì†Œí™”**. ê¸´ í…ìŠ¤íŠ¸ì˜ ì£¼ì œ ì „í™˜ ì²˜ë¦¬ì— ìœ ë¦¬.  \n",
    "**ë‹¨ì **: ì¤‘ë³µ ìž„ë² ë”©ìœ¼ë¡œ **ì²˜ë¦¬ëŸ‰ ì¦ê°€**.\n",
    "\n",
    "ì˜ˆë¬¸ì€ 3ë¬¸ìž¥ ìœˆë„ìš°, 1ë¬¸ìž¥ì”© ì´ë™(step=1)ìœ¼ë¡œ ë¶„í• í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1635bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n",
    "\n",
    "def simple_sent_split(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into sentences using simple rules.\n",
    "    - Split on periods, question marks, exclamation marks\n",
    "    - Split on line breaks\n",
    "    \"\"\"\n",
    "    # First split by line breaks\n",
    "    parts = re.split(r\"\\n+\", text.strip())\n",
    "    sentences = []\n",
    "    \n",
    "    for part in parts:\n",
    "        # Split on punctuation marks\n",
    "        split_sentences = re.split(r\"(?<=[\\.!\\?])\\s+\", part.strip())\n",
    "        # Filter out empty strings\n",
    "        split_sentences = [s for s in split_sentences if s]\n",
    "        sentences.extend(split_sentences)\n",
    "    \n",
    "    # Clean up: remove extra whitespace\n",
    "    return [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "def chunk_by_sliding_window(text: str, window_size: int = 10, overlap_ratio: float = 0.5) -> List[str]:\n",
    "    \"\"\"\n",
    "    Sliding window chunking: Create overlapping chunks by sliding a window across words.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to chunk\n",
    "        window_size: Number of words in each chunk\n",
    "        overlap_ratio: Ratio of overlap (0.0 = no overlap, 0.5 = 50% overlap, 0.8 = 80% overlap)\n",
    "    \n",
    "    Returns:\n",
    "        List of overlapping text chunks\n",
    "    \"\"\"\n",
    "    # Step 1: Split text into words\n",
    "    words = re.findall(r\"\\S+\", text)\n",
    "    \n",
    "    # Step 2: Calculate step size based on overlap ratio\n",
    "    # step = window_size * (1 - overlap_ratio)\n",
    "    step = int(window_size * (1 - overlap_ratio))\n",
    "    step = max(1, step)  # Ensure step is at least 1\n",
    "    \n",
    "    # Step 3: Create sliding windows\n",
    "    chunks = []\n",
    "    for i in range(0, len(words) - window_size + 1, step):\n",
    "        # Get a window of words\n",
    "        window_words = words[i:i+window_size]\n",
    "        # Join them into a chunk\n",
    "        chunk = \" \".join(window_words)\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def show_chunks(chunks):\n",
    "    \"\"\"Display chunks in a numbered format\"\"\"\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        print(f\"[{i}] {chunk}\\n\")\n",
    "\n",
    "# Example: Let's use sliding window on a text about data science\n",
    "text = \"\"\"\n",
    "Data science combines statistics and computer science. It helps us find patterns in large datasets. The goal is to extract meaningful insights from data.\n",
    "Machine learning is a key tool in data science. It can predict future outcomes based on past data. Popular algorithms include linear regression and decision trees.\n",
    "Data visualization makes insights easier to understand. Charts and graphs help communicate findings. Tools like matplotlib and seaborn are commonly used.\n",
    "Big data refers to datasets that are too large for traditional processing. Distributed computing frameworks like Hadoop help handle big data. Cloud platforms provide scalable storage and processing.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== Sliding Window Chunking (Word-based) ===\")\n",
    "print(\"Window size: 10 words, Overlap ratio: 0.5 (50% overlap)\")\n",
    "print(\"Notice how adjacent chunks share words for better context!\\n\")\n",
    "\n",
    "chunks = chunk_by_sliding_window(text, window_size=20, overlap_ratio=0.1)\n",
    "show_chunks(chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338a8444",
   "metadata": {},
   "source": [
    "\n",
    "## 4) êµ¬ì¡° ê¸°ë°˜ ì²­í‚¹ (Structure-aware)\n",
    "\n",
    "**ê°œë…**: ë¬¸ì„œì˜ **í˜•ì‹ì  êµ¬ì¡°**(ë§ˆí¬ë‹¤ìš´ í—¤ë”, ì½”ë“œë¸”ë¡, HTML íƒœê·¸ ë“±)ë¥¼ ê²½ê³„ë¡œ ë¶„í• í•©ë‹ˆë‹¤.  \n",
    "**ìž¥ì **: êµ¬í˜„ì´ ë¹„êµì  ê°„ë‹¨í•˜ë©° êµ¬ì¡° ì •ë³´ì— ê·¸ëŒ€ë¡œ ì˜ì¡´ ê°€ëŠ¥.  \n",
    "**ë‹¨ì **: êµ¬ì¡° ì¸ì‹ì´ ì–´ë ¤ìš´ ë¬¸ì„œì—ëŠ” í•œê³„ê°€ ìžˆê³ , ë™ì¼í•œ êµ¬ì¡°ë¼ë„ ë¬¸ì„œë§ˆë‹¤ ì˜ë¯¸ ì°¨ì´ê°€ ì¡´ìž¬í•  ìˆ˜ ìžˆìŒ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec2e7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Tuple\n",
    "\n",
    "def chunk_by_structure_markdown(text: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Structure-based chunking: Split text based on markdown headers and code blocks.\n",
    "    \n",
    "    Args:\n",
    "        text: Input markdown text to chunk\n",
    "    \n",
    "    Returns:\n",
    "        List of (title, content) tuples. Empty title if no header found.\n",
    "    \"\"\"\n",
    "    lines = text.splitlines()\n",
    "    chunks = []\n",
    "    current_content = []\n",
    "    current_title = \"\"\n",
    "    in_code_block = False\n",
    "    \n",
    "    for line in lines:\n",
    "        # Check if we're entering or exiting a code block\n",
    "        if line.strip().startswith(\"```\"):\n",
    "            in_code_block = not in_code_block\n",
    "            current_content.append(line)\n",
    "            continue\n",
    "        \n",
    "        # Check if this is a header (not inside code block)\n",
    "        if not in_code_block and re.match(r\"^\\s{0,3}#{1,6}\\s\", line):\n",
    "            # Save previous chunk if it has content\n",
    "            if current_content:\n",
    "                content = \"\\n\".join(current_content).strip()\n",
    "                chunks.append((current_title, content))\n",
    "                current_content = []\n",
    "            \n",
    "            # Extract title from header\n",
    "            current_title = re.sub(r\"^\\s{0,3}#{1,6}\\s*\", \"\", line).strip()\n",
    "        else:\n",
    "            # Regular content line\n",
    "            current_content.append(line)\n",
    "    \n",
    "    # Don't forget the last chunk\n",
    "    if current_content:\n",
    "        content = \"\\n\".join(current_content).strip()\n",
    "        chunks.append((current_title, content))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Example: Let's chunk a markdown document about Python programming\n",
    "markdown_text = \"\"\"\n",
    "# Python Basics\n",
    "\n",
    "Python is a high-level programming language. It's known for its simple syntax and readability. Python supports multiple programming paradigms.\n",
    "\n",
    "## Variables and Data Types\n",
    "\n",
    "Variables in Python don't need explicit declaration. You can assign values directly. Common data types include integers, floats, strings, and booleans.\n",
    "\n",
    "```python\n",
    "# Example of variable assignment\n",
    "name = \"Alice\"\n",
    "age = 25\n",
    "is_student = True\n",
    "```\n",
    "\n",
    "## Control Structures\n",
    "\n",
    "Python uses indentation to define code blocks. This makes the code more readable. Common control structures include if-else statements and loops.\n",
    "\n",
    "```python\n",
    "# Example of if-else statement\n",
    "if age >= 18:\n",
    "    print(\"Adult\")\n",
    "else:\n",
    "    print(\"Minor\")\n",
    "```\n",
    "\n",
    "## Functions\n",
    "\n",
    "Functions help organize code into reusable blocks. They can take parameters and return values. Python has many built-in functions.\n",
    "\n",
    "```python\n",
    "def greet(name):\n",
    "    return f\"Hello, {name}!\"\n",
    "```\n",
    "\n",
    "# Best Practices\n",
    "\n",
    "Good Python code follows certain conventions. Use meaningful variable names. Write clear comments. Keep functions small and focused.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== Structure-based Chunking ===\")\n",
    "print(\"This method splits text based on markdown headers and preserves code blocks.\\n\")\n",
    "\n",
    "blocks = chunk_by_structure_markdown(markdown_text)\n",
    "\n",
    "for i, (title, content) in enumerate(blocks, 1):\n",
    "    print(f\"[{i}] <{title or 'No Title'}>\")\n",
    "    print(content)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7e9831",
   "metadata": {},
   "source": [
    "\n",
    "## 5) ê³„ì¸µ êµ¬ì¡° ê¸°ë°˜ ì²­í‚¹ (Hierarchical)\n",
    "\n",
    "**ê°œë…**: ë¬¸ì„œì˜ ë…¼ë¦¬ì  **ê³„ì¸µ**ì„ ì¸ì‹í•´ ìƒìœ„/í•˜ìœ„ ë‚´ìš©ì„ í•¨ê»˜ ê´€ë¦¬í•©ë‹ˆë‹¤.  \n",
    "ê° ì²­í¬ê°€ ìƒìœ„ ì œëª©(ë§¥ë½)ì„ **í—¤ë”ë¡œ í¬í•¨**í•˜ë„ë¡ ì„¤ê³„í•˜ì—¬, ì„¹ì…˜ ê¸°ë°˜ ê²€ìƒ‰/ìš”ì•½/ì¸ë±ì‹±ì— ê°•í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946d31a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Tuple\n",
    "\n",
    "def simple_sent_split(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into sentences using simple rules.\n",
    "    - Split on periods, question marks, exclamation marks\n",
    "    - Split on line breaks\n",
    "    \"\"\"\n",
    "    # First split by line breaks\n",
    "    parts = re.split(r\"\\n+\", text.strip())\n",
    "    sentences = []\n",
    "    \n",
    "    for part in parts:\n",
    "        # Split on punctuation marks\n",
    "        split_sentences = re.split(r\"(?<=[\\.!\\?])\\s+\", part.strip())\n",
    "        # Filter out empty strings\n",
    "        split_sentences = [s for s in split_sentences if s]\n",
    "        sentences.extend(split_sentences)\n",
    "    \n",
    "    # Clean up: remove extra whitespace\n",
    "    return [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "def chunk_by_structure_markdown(text: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Structure-based chunking: Split text based on markdown headers and code blocks.\n",
    "    \n",
    "    Args:\n",
    "        text: Input markdown text to chunk\n",
    "    \n",
    "    Returns:\n",
    "        List of (title, content) tuples. Empty title if no header found.\n",
    "    \"\"\"\n",
    "    lines = text.splitlines()\n",
    "    chunks = []\n",
    "    current_content = []\n",
    "    current_title = \"\"\n",
    "    in_code_block = False\n",
    "    \n",
    "    for line in lines:\n",
    "        # Check if we're entering or exiting a code block\n",
    "        if line.strip().startswith(\"```\"):\n",
    "            in_code_block = not in_code_block\n",
    "            current_content.append(line)\n",
    "            continue\n",
    "        \n",
    "        # Check if this is a header (not inside code block)\n",
    "        if not in_code_block and re.match(r\"^\\s{0,3}#{1,6}\\s\", line):\n",
    "            # Save previous chunk if it has content\n",
    "            if current_content:\n",
    "                content = \"\\n\".join(current_content).strip()\n",
    "                chunks.append((current_title, content))\n",
    "                current_content = []\n",
    "            \n",
    "            # Extract title from header\n",
    "            current_title = re.sub(r\"^\\s{0,3}#{1,6}\\s*\", \"\", line).strip()\n",
    "        else:\n",
    "            # Regular content line\n",
    "            current_content.append(line)\n",
    "    \n",
    "    # Don't forget the last chunk\n",
    "    if current_content:\n",
    "        content = \"\\n\".join(current_content).strip()\n",
    "        chunks.append((current_title, content))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def chunk_hierarchical(text: str, section_depth: int = 2, leaf_sent_limit: int = 3) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Hierarchical chunking: Create chunks that preserve document structure.\n",
    "    \n",
    "    This method:\n",
    "    1. First splits by major sections (headers)\n",
    "    2. Then splits each section into smaller chunks by sentences\n",
    "    3. Each chunk includes the section title for context\n",
    "    \n",
    "    Args:\n",
    "        text: Input markdown text to chunk\n",
    "        section_depth: How deep to go in the hierarchy (not used in this simple version)\n",
    "        leaf_sent_limit: Maximum sentences per leaf chunk\n",
    "    \n",
    "    Returns:\n",
    "        List of (section_title, chunk_content) tuples\n",
    "    \"\"\"\n",
    "    # Step 1: Split by major sections using markdown headers\n",
    "    sections = chunk_by_structure_markdown(text)\n",
    "    \n",
    "    # Step 2: Split each section into smaller chunks\n",
    "    hierarchical_chunks = []\n",
    "    \n",
    "    for section_title, section_content in sections:\n",
    "        # Split section content into sentences\n",
    "        sentences = simple_sent_split(section_content)\n",
    "        \n",
    "        # Create chunks of sentences within this section\n",
    "        for i in range(0, len(sentences), leaf_sent_limit):\n",
    "            # Get a chunk of sentences\n",
    "            chunk_sentences = sentences[i:i+leaf_sent_limit]\n",
    "            chunk_content = \" \".join(chunk_sentences)\n",
    "            \n",
    "            # Store with section title for context\n",
    "            hierarchical_chunks.append((section_title, chunk_content))\n",
    "    \n",
    "    return hierarchical_chunks\n",
    "\n",
    "# Example: Let's create hierarchical chunks from a technical document\n",
    "document = \"\"\"\n",
    "# Introduction\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) combines search and generation. It enhances language models with external knowledge. The main advantage is improved accuracy and up-to-date information.\n",
    "\n",
    "# Architecture\n",
    "\n",
    "RAG systems have two main components. The retriever finds relevant documents. The generator creates responses based on retrieved content. This separation allows for better control and optimization.\n",
    "\n",
    "## Retrieval Component\n",
    "\n",
    "The retriever uses vector similarity search. Documents are converted to embeddings. Queries are also converted to embeddings. Similarity is measured using cosine distance.\n",
    "\n",
    "## Generation Component\n",
    "\n",
    "The generator is typically a large language model. It takes retrieved documents as context. The model generates responses based on this context. Fine-tuning can improve performance.\n",
    "\n",
    "# Implementation\n",
    "\n",
    "Building a RAG system requires several steps. First, prepare and index your documents. Second, implement the retrieval mechanism. Third, integrate with a language model.\n",
    "\n",
    "## Document Processing\n",
    "\n",
    "Documents need to be cleaned and chunked. Chunking strategies affect retrieval quality. Smaller chunks provide more precise matches. Larger chunks provide more context.\n",
    "\n",
    "## Vector Database\n",
    "\n",
    "A vector database stores document embeddings. Popular options include Pinecone and Weaviate. The database must support similarity search. Performance depends on indexing strategy.\n",
    "\n",
    "# Best Practices\n",
    "\n",
    "Good RAG systems follow certain principles. Use high-quality source documents. Implement proper chunking strategies. Monitor and evaluate system performance regularly.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== Hierarchical Chunking ===\")\n",
    "print(\"This method preserves document structure by keeping section titles with each chunk.\\n\")\n",
    "\n",
    "hierarchical_chunks = chunk_hierarchical(document, section_depth=2, leaf_sent_limit=2)\n",
    "\n",
    "for i, (section_title, chunk_content) in enumerate(hierarchical_chunks, 1):\n",
    "    print(f\"[{i}] <{section_title}>\")\n",
    "    print(chunk_content)\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
