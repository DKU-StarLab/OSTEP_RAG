{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "colab_intro",
   "metadata": {},
   "source": [
    "\n",
    "# Chapter 5: Colabì—ì„œ Ollamaë¡œ ê°„ë‹¨ Q&A\n",
    "\n",
    "Colab ëŸ°íƒ€ì„ì—ì„œ Ollamaë¥¼ ì„¤ì¹˜Â·êµ¬ë™í•˜ê³ , `llama3.1:8b` ëª¨ë¸ë¡œ ì‚¬ìš©ìì˜ ì˜ì–´ ì§ˆë¬¸ì„ í•œ ë²ˆì— ìƒì„±(ë¹„ìŠ¤íŠ¸ë¦¬ë°)í•˜ì—¬ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "\n",
    "## ğŸ“š í•™ìŠµ ëª©í‘œ\n",  
    "- Ollama ë¡œì»¬ REST API(`/api/version`, `/api/generate`) í˜¸ì¶œ íë¦„ ì´í•´\n",
    "- ìµœì†Œ ì…‹ì—…(ì„¤ì¹˜ â†’ ì„œë²„ êµ¬ë™ â†’ ëª¨ë¸ pull) ì ˆì°¨ ìŠµë“\n",
    "- ë‹¨ì¼ ì§ˆì˜ ì…ë ¥ â†’ ë¹„ìŠ¤íŠ¸ë¦¬ë° ë‹¨ì¼ ì‘ë‹µ ì¶œë ¥ íŒ¨í„´ ìµíˆê¸°\n",
    "\n",
    "\n",
    "## ğŸ“‹ ì‹¤ìŠµ êµ¬ì„±\n",
    "1) Google Colab í™˜ê²½ ì„¤ì •: Ollama ì„¤ì¹˜, ì„œë²„ ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰, ëª¨ë¸ pull\n",
    "2) í—¬ìŠ¤ ì²´í¬: `/api/version`ìœ¼ë¡œ ì„œë²„ ìƒíƒœ í™•ì¸\n",
    "3) í—¬í¼ í•¨ìˆ˜: `ollama_generate(prompt)` ë˜í¼\n",
    "4) ë‹¨ì¼ ìƒì„±: `input()`ìœ¼ë¡œ ì§ˆë¬¸ ë°›ê³  ì¦‰ì‹œ ì‘ë‹µ ì¶œë ¥\n",
    "\n",
    "> ì°¸ê³ : ë³¸ ì˜ˆì œëŠ” RAG/ëŒ€í™”/ìŠ¤íŠ¸ë¦¬ë°/íŒŒë¼ë¯¸í„° íŠœë‹ì„ ì œì™¸í•œ ìµœì†Œ êµ¬ì„±ì…ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4591a88",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## 1ï¸âƒ£ ì‹¤í–‰ ì „ ì¤€ë¹„\n",
    "\n",
    "ì´ ì…€ì—ì„œëŠ” Colab í™˜ê²½ì—ì„œ Ollamaë¥¼ ì„¤ì¹˜í•˜ê³  ì„œë²„ë¥¼ ë°±ê·¸ë¼ìš´ë“œë¡œ êµ¬ë™í•œ ë’¤, ì‚¬ìš©í•  ëª¨ë¸ì„ ë‚´ë ¤ë°›ìŠµë‹ˆë‹¤.\n",
    "\n",
    "**ì£¼ìš” ë‚´ìš©:**\n",
    "- Ollama ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰\n",
    "- `ollama serve` ë°±ê·¸ë¼ìš´ë“œ êµ¬ë™(ëŒ€ê¸° í¬í•¨)\n",
    "- `llama3.1:8b` ëª¨ë¸ pull\n",
    "\n",
    "**ì‹¤í–‰ ê²°ê³¼:**\n",
    "- ì„¤ì¹˜ ì™„ë£Œ ë©”ì‹œì§€ì™€ í•¨ê»˜ ì„œë²„/ëª¨ë¸ ì •ë³´ê°€ ì¶œë ¥ë©ë‹ˆë‹¤.\n",
    "- ë¡œì»¬ ì„œë²„ ì—”ë“œí¬ì¸íŠ¸: `http://localhost:11434`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colab_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "#  Google Colab í™˜ê²½ ì„¤ì •\n",
    "#    - Ollama ì„¤ì¹˜, ì„œë²„ ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰, ëª¨ë¸ pull\n",
    "# ========================================\n",
    "import subprocess, time\n",
    "\n",
    "# Ollama ì„¤ì¹˜\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "# Ollama ì„œë²„ ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰\n",
    "print(\"Starting Ollama server ...\")\n",
    "ollama_process = subprocess.Popen([\"ollama\", \"serve\"],\n",
    "                                  stdout=subprocess.DEVNULL,\n",
    "                                  stderr=subprocess.DEVNULL)\n",
    "# ì„œë²„ ì‹œì‘ ëŒ€ê¸°\n",
    "time.sleep(5)\n",
    "\n",
    "# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (llama3.1:8b)\n",
    "!ollama pull llama3.1:8b\n",
    "\n",
    "print(\"\\nâœ… Setup done.\")\n",
    "print(\"   Ollama server: http://localhost:11434\")\n",
    "print(\"   Model: llama3.1:8b (non-streaming)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f932c7",
   "metadata": {},
   "source": [
    "---\n",
    "## 2ï¸âƒ£ Quick Health Check\n",
    "\n",
    "ì´ ì…€ì—ì„œëŠ” Ollama ì„œë²„ì˜ ìƒíƒœë¥¼ ê°„ë‹¨íˆ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "\n",
    "**ì£¼ìš” ë‚´ìš©:**\n",
    "- `/api/version`ì— GET ìš”ì²­í•˜ì—¬ ì •ìƒ ì‘ë‹µ ì—¬ë¶€ í™•ì¸\n",
    "\n",
    "**ì‹¤í–‰ ê²°ê³¼:**\n",
    "- ì •ìƒ: ë²„ì „ JSON ì¶œë ¥(`{version: ...}`)\n",
    "- ì˜¤ë¥˜: ì˜ˆì™¸ ë©”ì‹œì§€ ë¬¸ìì—´ ì¶œë ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79fefb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 2ï¸âƒ£ Quick Health Check â€” /api/version\n",
    "# ========================================\n",
    "import requests\n",
    "\n",
    "def ollama_ok(host: str = \"http://localhost:11434\"):\n",
    "    try:\n",
    "        r = requests.get(host + \"/api/version\", timeout=5)\n",
    "        r.raise_for_status()\n",
    "        return True, r.json()\n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "ok, info = ollama_ok()\n",
    "print(\"Server ready:\", ok)\n",
    "print(\"Version or error:\", info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b36439f",
   "metadata": {},
   "source": [
    "---\n",
    "## 3ï¸âƒ£ Helper Functions\n",
    "\n",
    "ì´ ì…€ì—ì„œëŠ” ë‹¨ì¼(turn) ìƒì„±ìš© ë˜í¼ í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "**ì£¼ìš” ë‚´ìš©:**\n",
    "- `ollama_generate(prompt)`: `/api/generate`ì— ë¹„ìŠ¤íŠ¸ë¦¬ë° ìš”ì²­\n",
    "- ê³ ì • íŒŒë¼ë¯¸í„°(temperature/top_p/num_predict/repeat_penalty) ì‚¬ìš©\n",
    "\n",
    "**ì‹¤í–‰ ê²°ê³¼:**\n",
    "- ì„±ê³µ ì‹œ `response` í…ìŠ¤íŠ¸ ë°˜í™˜\n",
    "- ì‹¤íŒ¨ ì‹œ `[Error] ...` ë©”ì‹œì§€ ë°˜í™˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab430bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 3ï¸âƒ£ Helper â€” Single-turn generation wrapper\n",
    "#    - ollama_generate(prompt): ë¹„ìŠ¤íŠ¸ë¦¬ë° ë‹¨ì¼ ìƒì„± í˜¸ì¶œ\n",
    "# ========================================\n",
    "import requests, json\n",
    "\n",
    "OLLAMA_HOST = \"http://localhost:11434\"\n",
    "MODEL_NAME = \"llama3.1:8b\"\n",
    "\n",
    "# Fixed defaults for simple non-streaming Q&A\n",
    "_TEMPERATURE = 0.7\n",
    "_TOP_P = 0.9\n",
    "_NUM_PREDICT = 256\n",
    "_REPEAT_PENALTY = 1.1\n",
    "\n",
    "def ollama_generate(prompt: str) -> str:\n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"temperature\": _TEMPERATURE,\n",
    "            \"top_p\": _TOP_P,\n",
    "            \"num_predict\": _NUM_PREDICT,\n",
    "            \"repeat_penalty\": _REPEAT_PENALTY,\n",
    "        }\n",
    "    }\n",
    "    try:\n",
    "        r = requests.post(f\"{OLLAMA_HOST}/api/generate\", json=payload, timeout=600)\n",
    "        r.raise_for_status()\n",
    "        data = r.json() if isinstance(r.json(), dict) else json.loads(r.text)\n",
    "        return data.get(\"response\", \"\")\n",
    "    except Exception as e:\n",
    "        return f\"[Error] {e}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795d2375",
   "metadata": {},
   "source": [
    "---\n",
    "## 4ï¸âƒ£ Single-turn Generation\n",
    "\n",
    "ì´ ì…€ì—ì„œëŠ” ì‚¬ìš©ìì˜ ì˜ì–´ ì§ˆë¬¸ì„ ì…ë ¥ë°›ì•„ ë‹¨ì¼ ì‘ë‹µì„ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "**ì£¼ìš” ë‚´ìš©:**\n",
    "- `input()`ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ë¥¼ ë°›ìŒ\n",
    "- `ollama_generate()`ë¡œ ë‹¨ì¼(ë¹„ìŠ¤íŠ¸ë¦¬ë°) ìƒì„± ìˆ˜í–‰\n",
    "\n",
    "**ì‚¬ìš© ë°©ë²•:**\n",
    "- í”„ë¡¬í”„íŠ¸ì— ì˜ì–´ ë¬¸ì¥ì„ ì…ë ¥ í›„ Enterë¥¼ ëˆ„ë¥´ë©´ ë‹µë³€ì´ ì¶œë ¥ë©ë‹ˆë‹¤.\n",
    "- ë¹ˆ ì…ë ¥ ì‹œ ì•ˆë‚´ ë©”ì‹œì§€ë¥¼ ì¶œë ¥í•˜ê³  ì¢…ë£Œí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e13e4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 4ï¸âƒ£ Single-turn Generation â€” input() â†’ generate â†’ print\n",
    "# ========================================\n",
    "# Enter an English question and get a single response\n",
    "user_prompt = input(\"Enter your question (English): \")\n",
    "if user_prompt.strip():\n",
    "    print(ollama_generate(user_prompt))\n",
    "else:\n",
    "    print(\"[Info] Empty prompt.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
