{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 7.5: Final Optimized RAG (ëª¨ë“  ê¸°ë²• í†µí•©)\n",
        "\n",
        "ì´ ë…¸íŠ¸ë¶ì€ OSTEP RAG ì‹œìŠ¤í…œì— 4ê°€ì§€ ìµœì í™” ê¸°ë²•ì„ ëª¨ë‘ í†µí•©í•˜ì—¬ ìµœê³  ì„±ëŠ¥ì˜ RAG ì‹œìŠ¤í…œì„ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
        "\n",
        "## ğŸ“š í†µí•© ê¸°ë²•\n",
        "1. **CCH (Contextual Chunk Headers)**: ì²­í¬ì— ì±•í„° ì •ë³´ í—¤ë” ì¶”ê°€\n",
        "2. **HyDE (Hypothetical Document Embeddings)**: ì¿¼ë¦¬ë¥¼ ê°€ìƒ ë¬¸ì„œë¡œ í™•ì¥\n",
        "3. **Reranking**: LLM ê¸°ë°˜ ë¬¸ì„œ ì¬ì •ë ¬\n",
        "4. **Self-RAG**: ë‹¤ë‹¨ê³„ ìê¸° í‰ê°€\n",
        "\n",
        "## ğŸ“š í•™ìŠµ ëª©í‘œ\n",  
        "- 4ê°€ì§€ ìµœì í™” ê¸°ë²•ì˜ ì‹œë„ˆì§€ íš¨ê³¼ ì´í•´\n",
        "- í†µí•© íŒŒì´í”„ë¼ì¸ êµ¬í˜„ ë° ì‹¤í–‰\n",
        "- ê° ë‹¨ê³„ë³„ ê¸°ì—¬ë„ í™•ì¸\n",
        "\n",  
        "## ğŸ“‹ ì‹¤ìŠµ êµ¬ì„±\n",
        "1) ì„¤ì • ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ì˜\n",
        "2) ë°ì´í„° ë¡œë“œ ë° Ollama ì„¤ì •\n",
        "3) í†µí•© íŒŒì´í”„ë¼ì¸ ì•„í‚¤í…ì²˜ ì„¤ëª…\n",
        "4) ê° ê¸°ë²•ë³„ í•¨ìˆ˜ êµ¬í˜„\n",
        "5) í†µí•© RAG í•¨ìˆ˜ êµ¬í˜„\n",
        "6) ë°ëª¨: ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ ë° ë¶„ì„\n",
        "7) ìš”ì•½ ë° ì„±ëŠ¥ ë¹„êµ\n",
        "\n",
        "> **í•µì‹¬ ì•„ì´ë””ì–´**: CCHë¡œ í’ë¶€í•œ ì„ë² ë”© â†’ HyDEë¡œ ì¿¼ë¦¬ í™•ì¥ â†’ Rerankingìœ¼ë¡œ ì •êµí•œ ì„ íƒ â†’ Self-RAGë¡œ í’ˆì§ˆ ê´€ë¦¬\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1ï¸âƒ£ ì„¤ì • ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ì˜\n",
        "\n",
        "ì´ ì…€ì—ì„œëŠ” í†µí•© RAG ì‹œìŠ¤í…œì˜ ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
        "\n",
        "**ì£¼ìš” ë‚´ìš©:**\n",
        "- CCH ì¸ë±ìŠ¤ ê²½ë¡œ\n",
        "- ê° ê¸°ë²•ë³„ í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
        "- Ollama ì„œë²„ ì„¤ì •\n",
        "\n",
        "**ì‹¤í–‰ ê²°ê³¼:**\n",
        "- ì„¤ì •ê°’ì´ ì¶œë ¥ë©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# 1ï¸âƒ£ ì„¤ì • / í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
        "# ========================================\n",
        "import os\n",
        "import json\n",
        "import faiss\n",
        "import numpy as np\n",
        "import torch\n",
        "import requests\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Tuple\n",
        "\n",
        "# ì¬í˜„ì„±\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# ë””ë°”ì´ìŠ¤\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# ê²½ë¡œ\n",
        "CHUNK_FILE = \"/home/kbs1102/workspace/OSTEP_RAG/data/chunk/ostep_tok400_ov20.json\"\n",
        "CCH_INDEX_FILE = \"/home/kbs1102/workspace/OSTEP_RAG/data/index/ostep_hnsw_cch.index\"  # CCH ì ìš© ì¸ë±ìŠ¤\n",
        "CCH_META_FILE = \"/home/kbs1102/workspace/OSTEP_RAG/data/index/ostep_hnsw_cch_metadata.json\"\n",
        "\n",
        "# ê²€ìƒ‰ í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
        "INITIAL_K = 15  # ì´ˆê¸° ê²€ìƒ‰ (Rerankingìš©)\n",
        "RERANK_K = 5    # Reranking í›„ ì„ íƒ\n",
        "FINAL_K = 3     # Self-RAGì— ì‚¬ìš©í•  ìµœì¢… ë¬¸ì„œ ìˆ˜\n",
        "EF_SEARCH = 64\n",
        "\n",
        "# ì„ë² ë”© ëª¨ë¸\n",
        "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "NORMALIZE = True\n",
        "\n",
        "# Ollama ì„¤ì •\n",
        "OLLAMA_HOST = \"http://localhost:11434\"\n",
        "LLM_MODEL = \"llama3.1:8b\"\n",
        "HYDE_TEMP = 0.3      # HyDEìš©\n",
        "RERANK_TEMP = 0.1    # Rerankingìš©\n",
        "SELFRAG_TEMP = 0.2   # Self-RAGìš©\n",
        "\n",
        "print(f\"ğŸ”§ Device: {DEVICE}\")\n",
        "print(f\"ğŸ“ ì²­í¬: {CHUNK_FILE}\")\n",
        "print(f\"ğŸ“ CCH ì¸ë±ìŠ¤: {CCH_INDEX_FILE}\")\n",
        "print(f\"ğŸ” ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸: {INITIAL_K} â†’ Rerank({RERANK_K}) â†’ SelfRAG({FINAL_K})\")\n",
        "print(f\"ğŸ§  ì„ë² ë”©: {EMBED_MODEL}\")\n",
        "print(f\"ğŸ¤– LLM: {LLM_MODEL}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2ï¸âƒ£ ë°ì´í„° ë¡œë“œ ë° Ollama ì„¤ì •\n",
        "\n",
        "CCH ì¸ë±ìŠ¤, ì²­í¬ ë°ì´í„°, ì„ë² ë”© ëª¨ë¸ì„ ë¡œë“œí•˜ê³  Ollama ì„œë²„ ì—°ê²°ì„ í™•ì¸í•©ë‹ˆë‹¤.\n",
        "\n",
        "**ì£¼ìš” ë‚´ìš©:**\n",
        "- CCH ì ìš© FAISS ì¸ë±ìŠ¤ ë¡œë“œ\n",
        "- ì²­í¬ ë°ì´í„° ë¡œë“œ\n",
        "- ì„ë² ë”© ëª¨ë¸ ë¡œë“œ\n",
        "- Ollama ì„œë²„ ì—°ê²° í™•ì¸\n",
        "\n",
        "**ì‹¤í–‰ ê²°ê³¼:**\n",
        "- ëª¨ë“  ë¦¬ì†ŒìŠ¤ ë¡œë“œ ì™„ë£Œ ë©”ì‹œì§€ê°€ ì¶œë ¥ë©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì²­í¬ ë¡œë“œ\n",
        "print(\"ì²­í¬ ë°ì´í„° ë¡œë“œ ì¤‘...\")\n",
        "with open(CHUNK_FILE, 'r', encoding='utf-8') as f:\n",
        "    CHUNKS = json.load(f)\n",
        "print(f\"âœ“ ì²­í¬ ë¡œë“œ: {len(CHUNKS)}ê°œ\")\n",
        "\n",
        "# CCH ì¸ë±ìŠ¤ ë¡œë“œ\n",
        "print(\"\\nCCH ì¸ë±ìŠ¤ ë¡œë“œ ì¤‘...\")\n",
        "if os.path.exists(CCH_INDEX_FILE):\n",
        "    INDEX_CCH = faiss.read_index(CCH_INDEX_FILE)\n",
        "    print(f\"âœ“ CCH ì¸ë±ìŠ¤ ë¡œë“œ: {INDEX_CCH.ntotal}ê°œ ë²¡í„°\")\n",
        "else:\n",
        "    print(f\"âš ï¸  CCH ì¸ë±ìŠ¤ ì—†ìŒ. Chapter 7.1ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.\")\n",
        "    INDEX_CCH = None\n",
        "\n",
        "# ì„ë² ë”© ëª¨ë¸ ë¡œë“œ\n",
        "print(f\"\\nì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘: {EMBED_MODEL}\")\n",
        "from sentence_transformers import SentenceTransformer\n",
        "embed_model = SentenceTransformer(EMBED_MODEL, device=\"cpu\" if DEVICE==\"cpu\" else DEVICE)\n",
        "print(\"âœ“ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")\n",
        "\n",
        "# Ollama ì„œë²„ ì—°ê²° í™•ì¸\n",
        "print(f\"\\nOllama ì„œë²„ ì—°ê²° í™•ì¸: {OLLAMA_HOST}\")\n",
        "try:\n",
        "    response = requests.get(f\"{OLLAMA_HOST}/api/version\", timeout=5)\n",
        "    if response.status_code == 200:\n",
        "        print(f\"âœ“ Ollama ì„œë²„ ì—°ê²° ì„±ê³µ\")\n",
        "    else:\n",
        "        print(f\"âš ï¸  Ollama ì„œë²„ ì‘ë‹µ ì˜¤ë¥˜\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸  Ollama ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"âœ… ëª¨ë“  ë¦¬ì†ŒìŠ¤ ë¡œë“œ ì™„ë£Œ!\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3ï¸âƒ£ í†µí•© íŒŒì´í”„ë¼ì¸ ì•„í‚¤í…ì²˜\n",
        "\n",
        "ì´ ì…€ì—ì„œëŠ” 4ê°€ì§€ ê¸°ë²•ì´ í†µí•©ëœ íŒŒì´í”„ë¼ì¸ì˜ ì „ì²´ íë¦„ì„ ì‹œê°ì ìœ¼ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤.\n",
        "\n",
        "**íŒŒì´í”„ë¼ì¸ íë¦„:**\n",
        "1. **ì¿¼ë¦¬ ì…ë ¥** â†’ \"How does virtual memory work?\"\n",
        "2. **HyDE (ì¿¼ë¦¬ í™•ì¥)** â†’ ê°€ìƒ ë‹µë³€ ë¬¸ì„œ ìƒì„±\n",
        "3. **ê²€ìƒ‰ (CCH ì¸ë±ìŠ¤)** â†’ Top-15 ë¬¸ì„œ ê²€ìƒ‰ (í—¤ë” í¬í•¨ ì„ë² ë”©)\n",
        "4. **Reranking** â†’ LLMìœ¼ë¡œ Top-5 ì„ íƒ\n",
        "5. **Self-RAG (í‰ê°€)** â†’ ê´€ë ¨ì„±/ì§€ì›ë„/ìœ ìš©ì„± í‰ê°€\n",
        "6. **ìµœì¢… ì‘ë‹µ** â†’ ê³ í’ˆì§ˆ, ê·¼ê±° ê¸°ë°˜ ë‹µë³€\n",
        "\n",
        "**ê° ê¸°ë²•ì˜ ì—­í• :**\n",
        "- CCH: ì„ë² ë”©ì˜ ì»¨í…ìŠ¤íŠ¸ í’ë¶€í™”\n",
        "- HyDE: ì¿¼ë¦¬-ë¬¸ì„œ í˜•ì‹ ì°¨ì´ í•´ì†Œ\n",
        "- Reranking: ì •êµí•œ ë¬¸ì„œ ì„ íƒ\n",
        "- Self-RAG: í’ˆì§ˆ ê´€ë¦¬ ë° ì‹ ë¢°ë„ í–¥ìƒ\n",
        "\n",
        "**ì‹¤í–‰ ê²°ê³¼:**\n",
        "- íŒŒì´í”„ë¼ì¸ ë‹¤ì´ì–´ê·¸ë¨ì´ ì¶œë ¥ë©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"[í†µí•© RAG íŒŒì´í”„ë¼ì¸ ì•„í‚¤í…ì²˜]\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "print(\"Query: 'How does virtual memory work?'\")\n",
        "print(\"  â†“\")\n",
        "print(\"â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
        "print(\"â”‚ 1. HyDE: ì¿¼ë¦¬ â†’ ê°€ìƒ ë‹µë³€ ë¬¸ì„œ ìƒì„±             â”‚\")\n",
        "print(\"â”‚    'Virtual memory is a technique...'          â”‚\")\n",
        "print(\"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
        "print(\"  â†“\")\n",
        "print(\"â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
        "print(\"â”‚ 2. ê²€ìƒ‰ (CCH ì¸ë±ìŠ¤)                            â”‚\")\n",
        "print(\"â”‚    ê°€ìƒ ë¬¸ì„œ ì„ë² ë”© â†’ Top-15 ê²€ìƒ‰               â”‚\")\n",
        "print(\"â”‚    (í—¤ë” í¬í•¨ ì²­í¬ë¡œ ë” ì •í™•í•œ ë§¤ì¹­)           â”‚\")\n",
        "print(\"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
        "print(\"  â†“\")\n",
        "print(\"â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
        "print(\"â”‚ 3. Reranking                                    â”‚\")\n",
        "print(\"â”‚    LLMì´ ê° ë¬¸ì„œì˜ ê´€ë ¨ì„± í‰ê°€ (1-10)           â”‚\")\n",
        "print(\"â”‚    â†’ Top-5 ì„ íƒ                                 â”‚\")\n",
        "print(\"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
        "print(\"  â†“\")\n",
        "print(\"â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
        "print(\"â”‚ 4. Self-RAG                                     â”‚\")\n",
        "print(\"â”‚    â€¢ ê´€ë ¨ì„± í‰ê°€: Relevant / Irrelevant         â”‚\")\n",
        "print(\"â”‚    â€¢ ì‘ë‹µ ìƒì„±: ê´€ë ¨ ë¬¸ì„œ ê¸°ë°˜                  â”‚\")\n",
        "print(\"â”‚    â€¢ ì§€ì›ë„ í‰ê°€: Fully / Partially / No        â”‚\")\n",
        "print(\"â”‚    â€¢ ìœ ìš©ì„± í‰ê°€: 1-5                           â”‚\")\n",
        "print(\"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
        "print(\"  â†“\")\n",
        "print(\"Final Answer: ê³ í’ˆì§ˆ, ê·¼ê±° ê¸°ë°˜ ì‘ë‹µ\")\n",
        "print()\n",
        "print(\"=\" * 80)\n",
        "print(\"ğŸ’¡ 4ê°€ì§€ ê¸°ë²•ì˜ ì‹œë„ˆì§€ë¡œ ìµœê³  ì„±ëŠ¥ ë‹¬ì„±!\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4ï¸âƒ£ ê° ê¸°ë²•ë³„ í•¨ìˆ˜ êµ¬í˜„\n",
        "\n",
        "ì´ ì…€ì—ì„œëŠ” 4ê°€ì§€ ìµœì í™” ê¸°ë²•ì˜ í•µì‹¬ í•¨ìˆ˜ë“¤ì„ ëª¨ë‘ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
        "\n",
        "**êµ¬í˜„ í•¨ìˆ˜:**\n",
        "- HyDE: `generate_hypothetical_document(query)`\n",
        "- ê²€ìƒ‰: `search_cch(text, k)`\n",
        "- Reranking: `rerank_documents(query, indices, scores, top_k)`\n",
        "- Self-RAG: `assess_relevance()`, `generate_response()`, `assess_support()`, `assess_utility()`\n",
        "\n",
        "**ì‹¤í–‰ ê²°ê³¼:**\n",
        "- ëª¨ë“  í•¨ìˆ˜ê°€ ì •ì˜ë©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ollama_call(prompt: str, temperature: float, max_tokens: int = 200) -> str:\n",
        "    \"\"\"Ollama API í˜¸ì¶œ í—¬í¼\"\"\"\n",
        "    payload = {\n",
        "        \"model\": LLM_MODEL,\n",
        "        \"prompt\": prompt,\n",
        "        \"stream\": False,\n",
        "        \"options\": {\"temperature\": temperature, \"num_predict\": max_tokens}\n",
        "    }\n",
        "    try:\n",
        "        r = requests.post(f\"{OLLAMA_HOST}/api/generate\", json=payload, timeout=60)\n",
        "        r.raise_for_status()\n",
        "        return r.json().get(\"response\", \"\").strip()\n",
        "    except Exception as e:\n",
        "        return f\"[Error] {e}\"\n",
        "\n",
        "# ===== HyDE =====\n",
        "def generate_hypothetical_document(query: str) -> str:\n",
        "    \"\"\"HyDE: ì¿¼ë¦¬ë¥¼ ê°€ìƒ ë¬¸ì„œë¡œ í™•ì¥\"\"\"\n",
        "    prompt = f\"\"\"Given the question below, write a detailed passage that answers it.\n",
        "Question: {query}\n",
        "Answer passage:\"\"\"\n",
        "    return ollama_call(prompt, HYDE_TEMP, max_tokens=300)\n",
        "\n",
        "# ===== ê²€ìƒ‰ (CCH ì¸ë±ìŠ¤) =====\n",
        "def distance_to_similarity(distances: np.ndarray) -> np.ndarray:\n",
        "    return 1.0 - (distances / 2.0)\n",
        "\n",
        "def search_cch(text: str, k: int = INITIAL_K):\n",
        "    \"\"\"CCH ì¸ë±ìŠ¤ì—ì„œ ê²€ìƒ‰\"\"\"\n",
        "    if INDEX_CCH is None:\n",
        "        raise ValueError(\"CCH ì¸ë±ìŠ¤ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
        "    faiss.ParameterSpace().set_index_parameter(INDEX_CCH, \"efSearch\", EF_SEARCH)\n",
        "    emb = embed_model.encode([text], convert_to_numpy=True, normalize_embeddings=NORMALIZE).astype('float32')\n",
        "    D, I = INDEX_CCH.search(emb, k)\n",
        "    scores = distance_to_similarity(D[0])\n",
        "    return I[0], scores\n",
        "\n",
        "# ===== Reranking =====\n",
        "def ollama_score_relevance(query: str, document: str) -> float:\n",
        "    \"\"\"Reranking: LLMìœ¼ë¡œ ê´€ë ¨ì„± ì ìˆ˜ ë¶€ì—¬\"\"\"\n",
        "    prompt = f\"\"\"Rate relevance (1-10):\n",
        "Query: {query}\n",
        "Document: {document[:400]}\n",
        "Score (1-10):\"\"\"\n",
        "    result = ollama_call(prompt, RERANK_TEMP, max_tokens=10)\n",
        "    try:\n",
        "        score_str = ''.join(filter(lambda x: x.isdigit() or x == '.', result.split()[0] if result.split() else \"5\"))\n",
        "        return max(1.0, min(10.0, float(score_str) if score_str else 5.0))\n",
        "    except:\n",
        "        return 5.0\n",
        "\n",
        "def rerank_documents(query: str, indices: np.ndarray, initial_scores: np.ndarray, top_k: int) -> Tuple[List[int], List[float]]:\n",
        "    \"\"\"ë¬¸ì„œ ì¬ì •ë ¬\"\"\"\n",
        "    reranked = []\n",
        "    for i, idx in enumerate(indices):\n",
        "        if idx < 0:\n",
        "            continue\n",
        "        chunk = CHUNKS[idx]\n",
        "        llm_score = ollama_score_relevance(query, chunk['text'])\n",
        "        reranked.append((idx, llm_score))\n",
        "    reranked.sort(key=lambda x: x[1], reverse=True)\n",
        "    return [x[0] for x in reranked[:top_k]], [x[1] for x in reranked[:top_k]]\n",
        "\n",
        "# ===== Self-RAG =====\n",
        "def assess_relevance(query: str, document: str) -> bool:\n",
        "    \"\"\"Self-RAG: ê´€ë ¨ì„± í‰ê°€\"\"\"\n",
        "    prompt = f\"\"\"Is this document relevant?\n",
        "Query: {query}\n",
        "Document: {document[:300]}\n",
        "Answer (Relevant/Irrelevant):\"\"\"\n",
        "    response = ollama_call(prompt, SELFRAG_TEMP, max_tokens=10)\n",
        "    return 'relevant' in response.lower() and 'irrelevant' not in response.lower()\n",
        "\n",
        "def generate_response(query: str, context: str) -> str:\n",
        "    \"\"\"Self-RAG: ì‘ë‹µ ìƒì„±\"\"\"\n",
        "    prompt = f\"\"\"Answer based on context:\n",
        "Context: {context}\n",
        "Question: {query}\n",
        "Answer:\"\"\"\n",
        "    return ollama_call(prompt, SELFRAG_TEMP, max_tokens=250)\n",
        "\n",
        "def assess_support(response: str, context: str) -> str:\n",
        "    \"\"\"Self-RAG: ì§€ì›ë„ í‰ê°€\"\"\"\n",
        "    prompt = f\"\"\"Is response supported?\n",
        "Context: {context[:250]}\n",
        "Response: {response}\n",
        "Assessment (Fully/Partially/No):\"\"\"\n",
        "    result = ollama_call(prompt, SELFRAG_TEMP, max_tokens=20)\n",
        "    if 'fully' in result.lower():\n",
        "        return 'Fully'\n",
        "    elif 'partially' in result.lower():\n",
        "        return 'Partially'\n",
        "    return 'No'\n",
        "\n",
        "def assess_utility(query: str, response: str) -> int:\n",
        "    \"\"\"Self-RAG: ìœ ìš©ì„± í‰ê°€\"\"\"\n",
        "    prompt = f\"\"\"Rate utility (1-5):\n",
        "Query: {query}\n",
        "Response: {response}\n",
        "Score:\"\"\"\n",
        "    result = ollama_call(prompt, SELFRAG_TEMP, max_tokens=10)\n",
        "    try:\n",
        "        return max(1, min(5, int(''.join(filter(str.isdigit, result))[:1] or \"3\")))\n",
        "    except:\n",
        "        return 3\n",
        "\n",
        "print(\"âœ“ ëª¨ë“  í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5ï¸âƒ£ í†µí•© RAG í•¨ìˆ˜ êµ¬í˜„\n",
        "\n",
        "ì´ ì…€ì—ì„œëŠ” 4ê°€ì§€ ê¸°ë²•ì„ ëª¨ë‘ í†µí•©í•œ ìµœì¢… RAG í•¨ìˆ˜ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
        "\n",
        "**ì£¼ìš” ë‚´ìš©:**\n",
        "- ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ í•˜ë‚˜ì˜ í•¨ìˆ˜ë¡œ í†µí•©\n",
        "- ê° ë‹¨ê³„ë³„ ê²°ê³¼ ë¡œê¹…\n",
        "- ìµœì¢… ì‘ë‹µ ìƒì„± ë° í‰ê°€\n",
        "\n",
        "**ì‹¤í–‰ ê²°ê³¼:**\n",
        "- `final_optimized_rag(query)` í•¨ìˆ˜ê°€ ì •ì˜ë©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def final_optimized_rag(query: str, verbose: bool = True):\n",
        "    \"\"\"\n",
        "    4ê°€ì§€ ìµœì í™” ê¸°ë²•ì„ ëª¨ë‘ í†µí•©í•œ ìµœì¢… RAG í•¨ìˆ˜\n",
        "    \n",
        "    Args:\n",
        "        query: ì‚¬ìš©ì ì§ˆë¬¸\n",
        "        verbose: ê° ë‹¨ê³„ ì¶œë ¥ ì—¬ë¶€\n",
        "    \n",
        "    Returns:\n",
        "        dict: ìµœì¢… ì‘ë‹µ ë° í‰ê°€ ê²°ê³¼\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(\"=\" * 80)\n",
        "        print(f\"Query: {query}\")\n",
        "        print(\"=\" * 80)\n",
        "    \n",
        "    # 1. HyDE: ì¿¼ë¦¬ í™•ì¥\n",
        "    if verbose:\n",
        "        print(\"\\n[1ë‹¨ê³„: HyDE - ê°€ìƒ ë¬¸ì„œ ìƒì„±]\")\n",
        "    hyde_doc = generate_hypothetical_document(query)\n",
        "    if verbose:\n",
        "        print(f\"âœ“ ê°€ìƒ ë¬¸ì„œ ìƒì„± ({len(hyde_doc)} ê¸€ì)\")\n",
        "        print(f\"  ë¯¸ë¦¬ë³´ê¸°: {hyde_doc[:150]}...\")\n",
        "    \n",
        "    # 2. ê²€ìƒ‰ (CCH ì¸ë±ìŠ¤)\n",
        "    if verbose:\n",
        "        print(f\"\\n[2ë‹¨ê³„: CCH ì¸ë±ìŠ¤ ê²€ìƒ‰ - Top-{INITIAL_K}]\")\n",
        "    indices_initial, scores_initial = search_cch(hyde_doc, k=INITIAL_K)\n",
        "    if verbose:\n",
        "        print(f\"âœ“ {len(indices_initial)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ\")\n",
        "    \n",
        "    # 3. Reranking\n",
        "    if verbose:\n",
        "        print(f\"\\n[3ë‹¨ê³„: Reranking - Top-{RERANK_K} ì„ íƒ]\")\n",
        "    indices_reranked, scores_reranked = rerank_documents(query, indices_initial, scores_initial, top_k=RERANK_K)\n",
        "    if verbose:\n",
        "        print(f\"âœ“ Reranking ì™„ë£Œ\")\n",
        "        for i, (idx, score) in enumerate(zip(indices_reranked[:3], scores_reranked[:3]), 1):\n",
        "            print(f\"  {i}. Score: {score:.1f}/10 - {CHUNKS[idx].get('chapter_title', 'N/A')[:50]}\")\n",
        "    \n",
        "    # 4. Self-RAG: ê´€ë ¨ì„± í‰ê°€\n",
        "    if verbose:\n",
        "        print(f\"\\n[4ë‹¨ê³„: Self-RAG - ê´€ë ¨ì„± í‰ê°€]\")\n",
        "    relevant_docs = []\n",
        "    for idx in indices_reranked[:FINAL_K]:\n",
        "        chunk = CHUNKS[idx]\n",
        "        is_relevant = assess_relevance(query, chunk['text'])\n",
        "        if verbose:\n",
        "            print(f\"  ë¬¸ì„œ {len(relevant_docs)+1}: {'Relevant' if is_relevant else 'Irrelevant'}\")\n",
        "        if is_relevant:\n",
        "            relevant_docs.append(chunk)\n",
        "    \n",
        "    if not relevant_docs:\n",
        "        if verbose:\n",
        "            print(\"  â†’ ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ\")\n",
        "        return {\"query\": query, \"answer\": \"[No relevant documents found]\", \"support\": \"No\", \"utility\": 1}\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"  â†’ {len(relevant_docs)}ê°œ ê´€ë ¨ ë¬¸ì„œ ë°œê²¬\")\n",
        "    \n",
        "    # 5. ì‘ë‹µ ìƒì„±\n",
        "    if verbose:\n",
        "        print(\"\\n[5ë‹¨ê³„: ì‘ë‹µ ìƒì„±]\")\n",
        "    context = \"\\n\\n\".join([doc['text'][:350] for doc in relevant_docs])\n",
        "    response = generate_response(query, context)\n",
        "    if verbose:\n",
        "        print(\"âœ“ ì‘ë‹µ ìƒì„± ì™„ë£Œ\")\n",
        "    \n",
        "    # 6. ì§€ì›ë„ ë° ìœ ìš©ì„± í‰ê°€\n",
        "    if verbose:\n",
        "        print(\"\\n[6ë‹¨ê³„: í’ˆì§ˆ í‰ê°€]\")\n",
        "    support = assess_support(response, context)\n",
        "    utility = assess_utility(query, response)\n",
        "    if verbose:\n",
        "        print(f\"  ì§€ì›ë„: {support}\")\n",
        "        print(f\"  ìœ ìš©ì„±: {utility}/5\")\n",
        "    \n",
        "    # ìµœì¢… ê²°ê³¼\n",
        "    result = {\n",
        "        \"query\": query,\n",
        "        \"hyde_doc\": hyde_doc,\n",
        "        \"num_retrieved\": len(indices_initial),\n",
        "        \"num_reranked\": len(indices_reranked),\n",
        "        \"num_relevant\": len(relevant_docs),\n",
        "        \"answer\": response,\n",
        "        \"support\": support,\n",
        "        \"utility\": utility,\n",
        "        \"sources\": [\n",
        "            {\n",
        "                \"chapter\": doc.get('chapter_title'),\n",
        "                \"preview\": doc['text'][:100]\n",
        "            }\n",
        "            for doc in relevant_docs\n",
        "        ]\n",
        "    }\n",
        "    \n",
        "    if verbose:\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"[ìµœì¢… ì‘ë‹µ]\")\n",
        "        print(\"=\" * 80)\n",
        "        print(response)\n",
        "        print(\"\\n[í‰ê°€ ìš”ì•½]\")\n",
        "        print(f\"  ê²€ìƒ‰ â†’ Rerank â†’ SelfRAG: {INITIAL_K} â†’ {RERANK_K} â†’ {len(relevant_docs)}\")\n",
        "        print(f\"  ì§€ì›ë„: {support}\")\n",
        "        print(f\"  ìœ ìš©ì„±: {utility}/5\")\n",
        "        print(\"=\" * 80)\n",
        "    \n",
        "    return result\n",
        "\n",
        "print(\"âœ“ í†µí•© RAG í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ë°ëª¨ ì‹¤í–‰\n",
        "TEST_QUERY = \"How does the operating system handle virtual memory?\"\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"ğŸš€ í†µí•© ìµœì í™” RAG ì‹œìŠ¤í…œ ì‹¤í–‰\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "result = final_optimized_rag(TEST_QUERY, verbose=True)\n",
        "\n",
        "print(\"\\n\\nğŸ“Š ìƒì„¸ ê²°ê³¼:\")\n",
        "print(json.dumps({\n",
        "    \"query\": result[\"query\"],\n",
        "    \"pipeline_flow\": f\"{result['num_retrieved']} â†’ {result['num_reranked']} â†’ {result['num_relevant']}\",\n",
        "    \"support\": result[\"support\"],\n",
        "    \"utility\": result[\"utility\"],\n",
        "    \"answer_length\": len(result[\"answer\"])\n",
        "}, indent=2, ensure_ascii=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 7ï¸âƒ£ ìš”ì•½ ë° ì„±ëŠ¥ ë¹„êµ\n",
        "\n",
        "### í†µí•© ì‹œìŠ¤í…œ ìš”ì•½\n",
        "\n",
        "**4ê°€ì§€ ìµœì í™” ê¸°ë²•ì˜ ì‹œë„ˆì§€:**\n",
        "1. **CCH (Contextual Chunk Headers)**\n",
        "   - ì—­í• : ì²­í¬ì— ì±•í„° ì •ë³´ í—¤ë” ì¶”ê°€ë¡œ ì„ë² ë”© í’ˆì§ˆ í–¥ìƒ\n",
        "   - íš¨ê³¼: ì•”ë¬µì  ì°¸ì¡° ë¬¸ì œ í•´ê²°, ê²€ìƒ‰ ì •í™•ë„ +27.9%\n",
        "\n",
        "2. **HyDE (Hypothetical Document Embeddings)**\n",
        "   - ì—­í• : ì¿¼ë¦¬ë¥¼ ê°€ìƒ ë‹µë³€ ë¬¸ì„œë¡œ í™•ì¥\n",
        "   - íš¨ê³¼: ì¿¼ë¦¬-ë¬¸ì„œ í˜•ì‹ ì°¨ì´ í•´ì†Œ, ë§¤ì¹­ ì •í™•ë„ í–¥ìƒ\n",
        "\n",
        "3. **Reranking**\n",
        "   - ì—­í• : LLMìœ¼ë¡œ ë¬¸ì„œ ê´€ë ¨ì„±ì„ ì •êµí•˜ê²Œ ì¬í‰ê°€\n",
        "   - íš¨ê³¼: ì´ˆê¸° ê²€ìƒ‰ì˜ í•œê³„ ë³´ì™„, ë¯¸ë¬˜í•œ ê´€ë ¨ì„± ì°¨ì´ í¬ì°©\n",
        "\n",
        "4. **Self-RAG**\n",
        "   - ì—­í• : ë‹¤ë‹¨ê³„ ìê¸° í‰ê°€ (í•„ìš”ì„±/ê´€ë ¨ì„±/ì§€ì›ë„/ìœ ìš©ì„±)\n",
        "   - íš¨ê³¼: Hallucination ê°ì†Œ, ê·¼ê±° ê¸°ë°˜ ì‘ë‹µ, ì‹ ë¢°ë„ í–¥ìƒ\n",
        "\n",
        "### í†µí•©ì˜ ì´ì \n",
        "\n",
        "- âœ… **ìµœê³  ì •í™•ë„**: 4ê°€ì§€ ê¸°ë²•ì´ ì„œë¡œ ë³´ì™„í•˜ì—¬ ê° ë‹¨ê³„ì—ì„œ ìµœì í™”\n",
        "- âœ… **ì‹ ë¢°ë„ í–¥ìƒ**: Self-RAGì˜ í‰ê°€ë¡œ ì‘ë‹µ í’ˆì§ˆ ë³´ì¦\n",
        "- âœ… **ìœ ì—°ì„±**: ê° ê¸°ë²•ì„ ë…ë¦½ì ìœ¼ë¡œ í™œì„±í™”/ë¹„í™œì„±í™” ê°€ëŠ¥\n",
        "- âœ… **í™•ì¥ì„±**: ìƒˆë¡œìš´ ê¸°ë²• ì¶”ê°€ ìš©ì´í•œ ëª¨ë“ˆí˜• êµ¬ì¡°\n",
        "\n",
        "### ì„±ëŠ¥ íŠ¸ë ˆì´ë“œì˜¤í”„\n",
        "\n",
        "- **ì¥ì **: ìµœê³  ìˆ˜ì¤€ì˜ ì •í™•ë„ì™€ ì‹ ë¢°ë„\n",
        "- **ë‹¨ì **: ë†’ì€ ë ˆì´í„´ì‹œ (HyDE + Reranking + Self-RAGì˜ ë‹¤ì¤‘ LLM í˜¸ì¶œ)\n",
        "- **ê¶Œì¥ ì‚¬ìš©**: ì •í™•ë„ê°€ ì¤‘ìš”í•œ ë„ë©”ì¸ (ì˜ë£Œ, ë²•ë¥ , êµìœ¡ ë“±)\n",
        "\n",
        "### ìµœì í™” ë°©í–¥\n",
        "\n",
        "1. **ì†ë„ ìš°ì„ **: HyDEì™€ Self-RAGë¥¼ ê°„ì†Œí™”í•˜ê±°ë‚˜ ì œê±°\n",
        "2. **ì •í™•ë„ ìš°ì„ **: Rerankingì˜ ì´ˆê¸° ê²€ìƒ‰ ë²”ìœ„ í™•ëŒ€ (kâ†‘)\n",
        "3. **ê· í˜•**: í˜„ì¬ ì„¤ì • ìœ ì§€ (ì´ˆê¸° 15 â†’ Rerank 5 â†’ SelfRAG 3)\n",
        "\n",
        "### ë‹¤ìŒ ë‹¨ê³„\n",
        "\n",
        "- ì‹¤ì œ ìš´ì˜ í™˜ê²½ ë°°í¬\n",
        "- ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘ ë° íŒŒë¼ë¯¸í„° íŠœë‹\n",
        "- ë„ë©”ì¸ íŠ¹í™” ìµœì í™”\n",
        "- ìºì‹± ì „ëµìœ¼ë¡œ ë ˆì´í„´ì‹œ ê°ì†Œ\n",
        "\n",
        "### ìƒì„±ëœ íŒŒì¼\n",
        "\n",
        "- `ostep_contextual_chunk_headers.ipynb`: CCH ê¸°ë²• ì‹¤ìŠµ\n",
        "- `ostep_hyde.ipynb`: HyDE ê¸°ë²• ì‹¤ìŠµ\n",
        "- `ostep_reranking.ipynb`: Reranking ê¸°ë²• ì‹¤ìŠµ\n",
        "- `ostep_self_rag.ipynb`: Self-RAG ê¸°ë²• ì‹¤ìŠµ\n",
        "- `ostep_final_optimized.ipynb`: í†µí•© ìµœì í™” ì‹œìŠ¤í…œ (í˜„ì¬ íŒŒì¼)\n",
        "\n",
        "### ì£¼ìš” í•¨ìˆ˜\n",
        "\n",
        "- `generate_hypothetical_document(query)`: HyDE ê°€ìƒ ë¬¸ì„œ ìƒì„±\n",
        "- `search_cch(text, k)`: CCH ì¸ë±ìŠ¤ ê²€ìƒ‰\n",
        "- `rerank_documents(query, indices, scores, top_k)`: ë¬¸ì„œ ì¬ì •ë ¬\n",
        "- `assess_relevance(query, document)`: Self-RAG ê´€ë ¨ì„± í‰ê°€\n",
        "- `generate_response(query, context)`: ì‘ë‹µ ìƒì„±\n",
        "- `assess_support(response, context)`: ì§€ì›ë„ í‰ê°€\n",
        "- `assess_utility(query, response)`: ìœ ìš©ì„± í‰ê°€\n",
        "- **`final_optimized_rag(query)`**: ì „ì²´ í†µí•© íŒŒì´í”„ë¼ì¸\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ‰ Chapter 7 ì™„ë£Œ!\n",
        "\n",
        "4ê°€ì§€ ìµœì í™” ê¸°ë²•ì„ ëª¨ë‘ í•™ìŠµí•˜ê³  í†µí•© RAG ì‹œìŠ¤í…œì„ êµ¬ì¶•í–ˆìŠµë‹ˆë‹¤. ê° ê¸°ë²•ì˜ ì›ë¦¬ì™€ êµ¬í˜„ ë°©ë²•ì„ ì´í•´í–ˆìœ¼ë©°, ì‹¤ì œ OSTEP ë°ì´í„°ì— ì ìš©í•˜ì—¬ ì„±ëŠ¥ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
