{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 7.4: Self-RAG (ìê¸° í‰ê°€ ê¸°ë°˜ ê²€ìƒ‰)\n",
        "\n",
        "ì´ ë…¸íŠ¸ë¶ì€ OSTEP RAG ì‹œìŠ¤í…œì— Self-RAG ê¸°ë²•ì„ ì ìš©í•˜ì—¬ ê²€ìƒ‰ í’ˆì§ˆê³¼ ì‘ë‹µ ì‹ ë¢°ë„ë¥¼ ê°œì„ í•˜ëŠ” ë°©ë²•ì„ ì‹¤ìŠµí•©ë‹ˆë‹¤.\n",
        "\n",
        "## ğŸ“š í•™ìŠµ ëª©í‘œ\n",  
        "- Self-RAG ê°œë… ì´í•´: ê²€ìƒ‰ í•„ìš”ì„±, ê´€ë ¨ì„±, ì§€ì›ë„ë¥¼ ìë™ í‰ê°€\n",
        "- LLM ê¸°ë°˜ ë‹¤ë‹¨ê³„ í‰ê°€ í”„ë¡œì„¸ìŠ¤ êµ¬í˜„\n",
        "- Ollamaë¥¼ í™œìš©í•œ ë¡œì»¬ LLM ê¸°ë°˜ Self-RAG êµ¬í˜„\n", 
        "\n",
        "## ğŸ“‹ ì‹¤ìŠµ êµ¬ì„±\n",
        "1) ì„¤ì • ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ì˜\n",
        "2) ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ë° Ollama ì„¤ì •\n",
        "3) Self-RAG ê°œë… ì„¤ëª…\n",
        "4) Self-RAG í•¨ìˆ˜ êµ¬í˜„ (ê²€ìƒ‰ í•„ìš”ì„±/ê´€ë ¨ì„±/ì§€ì›ë„ í‰ê°€)\n",
        "5) ë°ëª¨: Self-RAG ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰\n",
        "6) ìš”ì•½ ë° ë‹¤ìŒ ë‹¨ê³„\n",
        "\n",
        "> **í•µì‹¬ ì•„ì´ë””ì–´**: RAG ì‹œìŠ¤í…œì´ ìŠ¤ìŠ¤ë¡œ \"ê²€ìƒ‰ì´ í•„ìš”í•œê°€?\", \"ê²€ìƒ‰ ê²°ê³¼ê°€ ê´€ë ¨ìˆëŠ”ê°€?\", \"ì‘ë‹µì´ ê·¼ê±° ê¸°ë°˜ì¸ê°€?\"ë¥¼ í‰ê°€í•˜ì—¬ ë” ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1ï¸âƒ£ ì„¤ì • ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ì˜\n",
        "\n",
        "ì´ ì…€ì—ì„œëŠ” ê²½ë¡œ, ëª¨ë¸, ê²€ìƒ‰ íŒŒë¼ë¯¸í„° ë“±ì„ ì •ì˜í•©ë‹ˆë‹¤.\n",
        "\n",
        "**ì£¼ìš” ë‚´ìš©:**\n",
        "- ê¸°ì¡´ ì‚°ì¶œë¬¼ ê²½ë¡œ (ì²­í¬ JSON, FAISS ì¸ë±ìŠ¤)\n",
        "- Ollama ì„œë²„ ì„¤ì • (Self-RAGìš© LLM)\n",
        "- Self-RAG íŒŒë¼ë¯¸í„°\n",
        "\n",
        "**ì‹¤í–‰ ê²°ê³¼:**\n",
        "- ì„¤ì •ê°’ì´ ì¶œë ¥ë©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# 1ï¸âƒ£ ì„¤ì • / í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
        "# ========================================\n",
        "import os\n",
        "import json\n",
        "import faiss\n",
        "import numpy as np\n",
        "import torch\n",
        "import requests\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Tuple\n",
        "\n",
        "# ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ì„¤ì •\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# ë””ë°”ì´ìŠ¤\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# ê²½ë¡œ (ê¸°ì¡´ ì‚°ì¶œë¬¼)\n",
        "CHUNK_FILE = \"/home/kbs1102/workspace/OSTEP_RAG/data/chunk/ostep_tok400_ov20.json\"\n",
        "INDEX_FILE = \"/home/kbs1102/workspace/OSTEP_RAG/data/index/ostep_hnsw.index\"\n",
        "\n",
        "# ê²€ìƒ‰ í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
        "TOP_K = 3\n",
        "EF_SEARCH = 64\n",
        "\n",
        "# ì„ë² ë”© ëª¨ë¸\n",
        "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "NORMALIZE = True\n",
        "\n",
        "# Ollama ì„¤ì • (Self-RAGìš© LLM)\n",
        "OLLAMA_HOST = \"http://localhost:11434\"\n",
        "LLM_MODEL = \"llama3.1:8b\"\n",
        "TEMPERATURE = 0.2  # ë‚®ì€ temperatureë¡œ ì¼ê´€ëœ í‰ê°€\n",
        "\n",
        "print(f\"ğŸ”§ Device: {DEVICE}\")\n",
        "print(f\"ğŸ“ ì²­í¬ íŒŒì¼: {CHUNK_FILE}\")\n",
        "print(f\"ğŸ“ ì¸ë±ìŠ¤ íŒŒì¼: {INDEX_FILE}\")\n",
        "print(f\"ğŸ” TOP_K={TOP_K}\")\n",
        "print(f\"ğŸ§  ì„ë² ë”© ëª¨ë¸: {EMBED_MODEL}\")\n",
        "print(f\"ğŸ¤– LLM: {LLM_MODEL} @ {OLLAMA_HOST}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2ï¸âƒ£ ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ë° Ollama ì„¤ì •\n",
        "\n",
        "Chapter 2-4ì—ì„œ ìƒì„±í•œ ì²­í¬ JSONê³¼ FAISS ì¸ë±ìŠ¤ë¥¼ ë¡œë“œí•˜ê³ , Ollama ì„œë²„ ì—°ê²°ì„ í™•ì¸í•©ë‹ˆë‹¤.\n",
        "\n",
        "**ì£¼ìš” ë‚´ìš©:**\n",
        "- ì²­í¬ ë°ì´í„° ë¡œë“œ\n",
        "- FAISS ì¸ë±ìŠ¤ ë¡œë“œ\n",
        "- ì„ë² ë”© ëª¨ë¸ ë¡œë“œ\n",
        "- Ollama ì„œë²„ ì—°ê²° í™•ì¸\n",
        "\n",
        "**ì‹¤í–‰ ê²°ê³¼:**\n",
        "- ë°ì´í„° ë¡œë“œ ì™„ë£Œ ë©”ì‹œì§€ì™€ Ollama ì„œë²„ ìƒíƒœê°€ ì¶œë ¥ë©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì²­í¬ ë°ì´í„° ë¡œë“œ\n",
        "print(\"ì²­í¬ ë°ì´í„° ë¡œë“œ ì¤‘...\")\n",
        "with open(CHUNK_FILE, 'r', encoding='utf-8') as f:\n",
        "    CHUNKS = json.load(f)\n",
        "print(f\"âœ“ ì²­í¬ ë¡œë“œ ì™„ë£Œ: {len(CHUNKS)}ê°œ\")\n",
        "\n",
        "# ì¸ë±ìŠ¤ ë¡œë“œ\n",
        "print(\"\\nì¸ë±ìŠ¤ ë¡œë“œ ì¤‘...\")\n",
        "INDEX = faiss.read_index(INDEX_FILE)\n",
        "print(f\"âœ“ ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ: {INDEX.ntotal}ê°œ ë²¡í„°\")\n",
        "\n",
        "# ì„ë² ë”© ëª¨ë¸ ë¡œë“œ\n",
        "print(f\"\\nì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘: {EMBED_MODEL}\")\n",
        "from sentence_transformers import SentenceTransformer\n",
        "embed_model = SentenceTransformer(EMBED_MODEL, device=\"cpu\" if DEVICE==\"cpu\" else DEVICE)\n",
        "print(\"âœ“ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")\n",
        "\n",
        "# Ollama ì„œë²„ ì—°ê²° í™•ì¸\n",
        "print(f\"\\nOllama ì„œë²„ ì—°ê²° í™•ì¸: {OLLAMA_HOST}\")\n",
        "try:\n",
        "    response = requests.get(f\"{OLLAMA_HOST}/api/version\", timeout=5)\n",
        "    if response.status_code == 200:\n",
        "        print(f\"âœ“ Ollama ì„œë²„ ì—°ê²° ì„±ê³µ\")\n",
        "    else:\n",
        "        print(f\"âš ï¸  Ollama ì„œë²„ ì‘ë‹µ ì˜¤ë¥˜: {response.status_code}\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸  Ollama ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}\")\n",
        "    print(\"   Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”: ollama serve\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3ï¸âƒ£ Self-RAG ê°œë… ì„¤ëª…\n",
        "\n",
        "ì´ ì…€ì—ì„œëŠ” Self-RAGì˜ í•µì‹¬ ê°œë…ì„ ì„¤ëª…í•©ë‹ˆë‹¤.\n",
        "\n",
        "**Self-RAGë€?**\n",
        "- **ë¬¸ì œ**: RAG ì‹œìŠ¤í…œì´ í•­ìƒ ê²€ìƒ‰ì´ í•„ìš”í•œì§€, ê²€ìƒ‰ ê²°ê³¼ê°€ ê´€ë ¨ìˆëŠ”ì§€, ë‹µë³€ì´ ê·¼ê±° ê¸°ë°˜ì¸ì§€ ëª¨ë¦„\n",
        "- **í•´ê²°ì±…**: ë‹¤ë‹¨ê³„ ìê¸° í‰ê°€ í”„ë¡œì„¸ìŠ¤\n",
        "  1. **ê²€ìƒ‰ í•„ìš”ì„± í‰ê°€**: ì´ ì§ˆë¬¸ì— ê²€ìƒ‰ì´ í•„ìš”í•œê°€? (Yes/No)\n",
        "  2. **ê´€ë ¨ì„± í‰ê°€**: ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì§ˆë¬¸ê³¼ ê´€ë ¨ìˆëŠ”ê°€? (Relevant/Irrelevant)\n",
        "  3. **ì‘ë‹µ ìƒì„±**: ê´€ë ¨ ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ìƒì„±\n",
        "  4. **ì§€ì›ë„ í‰ê°€**: ë‹µë³€ì´ ë¬¸ì„œì— ì˜í•´ ë’·ë°›ì¹¨ë˜ëŠ”ê°€? (Fully/Partially/No)\n",
        "  5. **ìœ ìš©ì„± í‰ê°€**: ë‹µë³€ì´ ì§ˆë¬¸ì— ìœ ìš©í•œê°€? (1-5)\n",
        "- **íš¨ê³¼**: ë” ì‹ ë¢°í•  ìˆ˜ ìˆê³  ê·¼ê±° ê¸°ë°˜ì˜ ì‘ë‹µ ìƒì„±\n",
        "\n",
        "**Self-RAGì˜ ì¥ì :**\n",
        "- ë¶ˆí•„ìš”í•œ ê²€ìƒ‰ ë°©ì§€\n",
        "- ê´€ë ¨ ì—†ëŠ” ë¬¸ì„œ í•„í„°ë§\n",
        "- ê·¼ê±° ì—†ëŠ” hallucination ê°ì†Œ\n",
        "- ì‘ë‹µ í’ˆì§ˆ ìë™ í‰ê°€\n",
        "\n",
        "**ì‹¤í–‰ ê²°ê³¼:**\n",
        "- ê°œë… ì„¤ëª…ì´ ì¶œë ¥ë©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"[Self-RAG ê°œë…]\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "print(\"Self-RAG í”„ë¡œì„¸ìŠ¤:\")\n",
        "print()\n",
        "print(\"1. ê²€ìƒ‰ í•„ìš”ì„± í‰ê°€\")\n",
        "print(\"   Q: '2+2ëŠ” ì–¼ë§ˆ?' â†’ A: 'No' (ê²€ìƒ‰ ë¶ˆí•„ìš”)\")\n",
        "print(\"   Q: 'Virtual memoryë€?' â†’ A: 'Yes' (ê²€ìƒ‰ í•„ìš”)\")\n",
        "print()\n",
        "print(\"2. ê²€ìƒ‰ ìˆ˜í–‰ (í•„ìš”í•œ ê²½ìš°)\")\n",
        "print(\"   â†’ Top-K ë¬¸ì„œ ê²€ìƒ‰\")\n",
        "print()\n",
        "print(\"3. ê´€ë ¨ì„± í‰ê°€\")\n",
        "print(\"   ê° ë¬¸ì„œ: Relevant / Irrelevant\")\n",
        "print()\n",
        "print(\"4. ì‘ë‹µ ìƒì„±\")\n",
        "print(\"   ê´€ë ¨ ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ìƒì„±\")\n",
        "print()\n",
        "print(\"5. ì§€ì›ë„ í‰ê°€\")\n",
        "print(\"   ë‹µë³€ì´ ë¬¸ì„œì— ì˜í•´ ë’·ë°›ì¹¨ë˜ëŠ”ê°€?\")\n",
        "print(\"   â†’ Fully supported / Partially / No support\")\n",
        "print()\n",
        "print(\"6. ìœ ìš©ì„± í‰ê°€\")\n",
        "print(\"   ë‹µë³€ì´ ì§ˆë¬¸ì— ì–¼ë§ˆë‚˜ ìœ ìš©í•œê°€? (1-5)\")\n",
        "print()\n",
        "print(\"=\" * 80)\n",
        "print(\"ğŸ’¡ í•µì‹¬: ì‹œìŠ¤í…œì´ ìŠ¤ìŠ¤ë¡œ í’ˆì§ˆì„ í‰ê°€í•˜ì—¬ ì‹ ë¢°ë„ í–¥ìƒ\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4ï¸âƒ£ Self-RAG í•¨ìˆ˜ êµ¬í˜„\n",
        "\n",
        "ì´ ì…€ì—ì„œëŠ” Ollamaë¥¼ ì‚¬ìš©í•˜ì—¬ Self-RAGì˜ ê° í‰ê°€ ë‹¨ê³„ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
        "\n",
        "**ì£¼ìš” ë‚´ìš©:**\n",
        "- ê²€ìƒ‰ í•„ìš”ì„± í‰ê°€ í•¨ìˆ˜\n",
        "- ê´€ë ¨ì„± í‰ê°€ í•¨ìˆ˜\n",
        "- ì‘ë‹µ ìƒì„± í•¨ìˆ˜\n",
        "- ì§€ì›ë„ í‰ê°€ í•¨ìˆ˜\n",
        "- ìœ ìš©ì„± í‰ê°€ í•¨ìˆ˜\n",
        "- ì „ì²´ Self-RAG í†µí•© í•¨ìˆ˜\n",
        "\n",
        "**ì‹¤í–‰ ê²°ê³¼:**\n",
        "- ê° í‰ê°€ í•¨ìˆ˜ê°€ ì •ì˜ë©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ollama_call(prompt: str, max_tokens: int = 50) -> str:\n",
        "    \"\"\"Ollama API í˜¸ì¶œ í—¬í¼ í•¨ìˆ˜\"\"\"\n",
        "    payload = {\n",
        "        \"model\": LLM_MODEL,\n",
        "        \"prompt\": prompt,\n",
        "        \"stream\": False,\n",
        "        \"options\": {\n",
        "            \"temperature\": TEMPERATURE,\n",
        "            \"num_predict\": max_tokens,\n",
        "        }\n",
        "    }\n",
        "    try:\n",
        "        r = requests.post(f\"{OLLAMA_HOST}/api/generate\", json=payload, timeout=30)\n",
        "        r.raise_for_status()\n",
        "        return r.json().get(\"response\", \"\").strip()\n",
        "    except Exception as e:\n",
        "        return f\"[Error] {e}\"\n",
        "\n",
        "# 1. ê²€ìƒ‰ í•„ìš”ì„± í‰ê°€\n",
        "def assess_retrieval_need(query: str) -> bool:\n",
        "    \"\"\"ê²€ìƒ‰ì´ í•„ìš”í•œì§€ í‰ê°€\"\"\"\n",
        "    prompt = f\"\"\"Does the following question require retrieval of external information to answer?\n",
        "Output ONLY 'Yes' or 'No', nothing else.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer (Yes/No):\"\"\"\n",
        "    response = ollama_call(prompt, max_tokens=10)\n",
        "    return 'yes' in response.lower()\n",
        "\n",
        "# 2. ê´€ë ¨ì„± í‰ê°€\n",
        "def assess_relevance(query: str, document: str) -> bool:\n",
        "    \"\"\"ë¬¸ì„œê°€ ì¿¼ë¦¬ì™€ ê´€ë ¨ìˆëŠ”ì§€ í‰ê°€\"\"\"\n",
        "    prompt = f\"\"\"Is the following document relevant to answering the question?\n",
        "Output ONLY 'Relevant' or 'Irrelevant', nothing else.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Document: {document[:400]}\n",
        "\n",
        "Answer (Relevant/Irrelevant):\"\"\"\n",
        "    response = ollama_call(prompt, max_tokens=10)\n",
        "    return 'relevant' in response.lower() and 'irrelevant' not in response.lower()\n",
        "\n",
        "# 3. ì‘ë‹µ ìƒì„±\n",
        "def generate_response(query: str, context: str) -> str:\n",
        "    \"\"\"ë¬¸ì„œ ê¸°ë°˜ ì‘ë‹µ ìƒì„±\"\"\"\n",
        "    prompt = f\"\"\"Answer the following question based on the provided context.\n",
        "Be concise and accurate.\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer:\"\"\"\n",
        "    response = ollama_call(prompt, max_tokens=200)\n",
        "    return response\n",
        "\n",
        "# 4. ì§€ì›ë„ í‰ê°€\n",
        "def assess_support(response: str, context: str) -> str:\n",
        "    \"\"\"ì‘ë‹µì´ ì»¨í…ìŠ¤íŠ¸ì— ì˜í•´ ë’·ë°›ì¹¨ë˜ëŠ”ì§€ í‰ê°€\"\"\"\n",
        "    prompt = f\"\"\"Is the response fully supported by the context?\n",
        "Output ONLY one of: 'Fully supported', 'Partially supported', or 'No support'.\n",
        "\n",
        "Context: {context[:300]}\n",
        "\n",
        "Response: {response}\n",
        "\n",
        "Assessment:\"\"\"\n",
        "    result = ollama_call(prompt, max_tokens=20)\n",
        "    if 'fully' in result.lower():\n",
        "        return 'Fully supported'\n",
        "    elif 'partially' in result.lower():\n",
        "        return 'Partially supported'\n",
        "    else:\n",
        "        return 'No support'\n",
        "\n",
        "# 5. ìœ ìš©ì„± í‰ê°€\n",
        "def assess_utility(query: str, response: str) -> int:\n",
        "    \"\"\"ì‘ë‹µì˜ ìœ ìš©ì„±ì„ 1-5ë¡œ í‰ê°€\"\"\"\n",
        "    prompt = f\"\"\"Rate how useful this response is for answering the question.\n",
        "Output ONLY a number from 1 to 5, nothing else.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Response: {response}\n",
        "\n",
        "Utility score (1-5):\"\"\"\n",
        "    result = ollama_call(prompt, max_tokens=10)\n",
        "    try:\n",
        "        score = int(''.join(filter(str.isdigit, result))[:1] or \"3\")\n",
        "        return max(1, min(5, score))\n",
        "    except:\n",
        "        return 3\n",
        "\n",
        "print(\"âœ“ Self-RAG í‰ê°€ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5ï¸âƒ£ ë°ëª¨: Self-RAG ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰\n",
        "\n",
        "ì´ ì…€ì—ì„œëŠ” Self-RAGì˜ ì „ì²´ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹¤í–‰í•˜ê³  ê° ë‹¨ê³„ì˜ ê²°ê³¼ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\n",
        "\n",
        "**ì£¼ìš” ë‚´ìš©:**\n",
        "- ê²€ìƒ‰ í•„ìš”ì„± í‰ê°€\n",
        "- ê²€ìƒ‰ ìˆ˜í–‰ \n",
        "- ê´€ë ¨ì„± í‰ê°€\n",
        "- ì‘ë‹µ ìƒì„±\n",
        "- ì§€ì›ë„ ë° ìœ ìš©ì„± í‰ê°€\n",
        "\n",
        "**ì‹¤í–‰ ê²°ê³¼:**\n",
        "- ê° ë‹¨ê³„ì˜ í‰ê°€ ê²°ê³¼ì™€ ìµœì¢… ì‘ë‹µì´ ì¶œë ¥ë©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def distance_to_similarity(distances: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"L2 ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜\"\"\"\n",
        "    return 1.0 - (distances / 2.0)\n",
        "\n",
        "def search(query: str, k: int = TOP_K):\n",
        "    \"\"\"ì„ë² ë”© ê¸°ë°˜ ê²€ìƒ‰\"\"\"\n",
        "    faiss.ParameterSpace().set_index_parameter(INDEX, \"efSearch\", EF_SEARCH)\n",
        "    q_emb = embed_model.encode([query], convert_to_numpy=True, normalize_embeddings=NORMALIZE).astype('float32')\n",
        "    D, I = INDEX.search(q_emb, k)\n",
        "    scores = distance_to_similarity(D[0])\n",
        "    return I[0], scores\n",
        "\n",
        "def self_rag(query: str):\n",
        "    \"\"\"Self-RAG ì „ì²´ í”„ë¡œì„¸ìŠ¤\"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Query: {query}\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # 1. ê²€ìƒ‰ í•„ìš”ì„± í‰ê°€\n",
        "    print(\"\\n[1ë‹¨ê³„: ê²€ìƒ‰ í•„ìš”ì„± í‰ê°€]\")\n",
        "    need_retrieval = assess_retrieval_need(query)\n",
        "    print(f\"ê²€ìƒ‰ í•„ìš”? {'Yes' if need_retrieval else 'No'}\")\n",
        "    \n",
        "    if not need_retrieval:\n",
        "        print(\"\\nê²€ìƒ‰ ì—†ì´ ì§ì ‘ ë‹µë³€ ìƒì„±...\")\n",
        "        response = generate_response(query, \"No specific context provided.\")\n",
        "        print(f\"\\n[ìµœì¢… ì‘ë‹µ]\\n{response}\")\n",
        "        return response\n",
        "    \n",
        "    # 2. ê²€ìƒ‰ ìˆ˜í–‰\n",
        "    print(\"\\n[2ë‹¨ê³„: ê²€ìƒ‰ ìˆ˜í–‰]\")\n",
        "    indices, scores = search(query, k=TOP_K)\n",
        "    print(f\"âœ“ {len(indices)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ\")\n",
        "    \n",
        "    # 3. ê´€ë ¨ì„± í‰ê°€\n",
        "    print(\"\\n[3ë‹¨ê³„: ê´€ë ¨ì„± í‰ê°€]\")\n",
        "    relevant_docs = []\n",
        "    for i, idx in enumerate(indices):\n",
        "        if idx < 0:\n",
        "            continue\n",
        "        chunk = CHUNKS[idx]\n",
        "        is_relevant = assess_relevance(query, chunk['text'])\n",
        "        print(f\"  ë¬¸ì„œ {i+1}: {'Relevant' if is_relevant else 'Irrelevant'}\")\n",
        "        if is_relevant:\n",
        "            relevant_docs.append(chunk)\n",
        "    \n",
        "    if not relevant_docs:\n",
        "        print(\"  â†’ ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ. ê²€ìƒ‰ ì—†ì´ ë‹µë³€ ìƒì„±...\")\n",
        "        response = generate_response(query, \"No relevant context found.\")\n",
        "        print(f\"\\n[ìµœì¢… ì‘ë‹µ]\\n{response}\")\n",
        "        return response\n",
        "    \n",
        "    print(f\"  â†’ {len(relevant_docs)}ê°œ ê´€ë ¨ ë¬¸ì„œ ë°œê²¬\")\n",
        "    \n",
        "    # 4. ì‘ë‹µ ìƒì„±\n",
        "    print(\"\\n[4ë‹¨ê³„: ì‘ë‹µ ìƒì„±]\")\n",
        "    context = \"\\n\\n\".join([doc['text'][:300] for doc in relevant_docs])\n",
        "    response = generate_response(query, context)\n",
        "    print(f\"âœ“ ì‘ë‹µ ìƒì„± ì™„ë£Œ\")\n",
        "    \n",
        "    # 5. ì§€ì›ë„ í‰ê°€\n",
        "    print(\"\\n[5ë‹¨ê³„: ì§€ì›ë„ í‰ê°€]\")\n",
        "    support = assess_support(response, context)\n",
        "    print(f\"ì§€ì›ë„: {support}\")\n",
        "    \n",
        "    # 6. ìœ ìš©ì„± í‰ê°€\n",
        "    print(\"\\n[6ë‹¨ê³„: ìœ ìš©ì„± í‰ê°€]\")\n",
        "    utility = assess_utility(query, response)\n",
        "    print(f\"ìœ ìš©ì„±: {utility}/5\")\n",
        "    \n",
        "    # ìµœì¢… ì¶œë ¥\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"[ìµœì¢… ì‘ë‹µ]\")\n",
        "    print(\"=\" * 80)\n",
        "    print(response)\n",
        "    print(\"\\n[í‰ê°€ ìš”ì•½]\")\n",
        "    print(f\"  - ê´€ë ¨ ë¬¸ì„œ ìˆ˜: {len(relevant_docs)}\")\n",
        "    print(f\"  - ì§€ì›ë„: {support}\")\n",
        "    print(f\"  - ìœ ìš©ì„±: {utility}/5\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    return response\n",
        "\n",
        "# ë°ëª¨ ì‹¤í–‰\n",
        "TEST_QUERY = \"How does the operating system handle virtual memory?\"\n",
        "self_rag(TEST_QUERY)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6ï¸âƒ£ ì¶”ê°€ í…ŒìŠ¤íŠ¸: ê²€ìƒ‰ì´ ë¶ˆí•„ìš”í•œ ì¿¼ë¦¬\n",
        "\n",
        "ì´ ì…€ì—ì„œëŠ” ê²€ìƒ‰ì´ ë¶ˆí•„ìš”í•œ ê°„ë‹¨í•œ ì§ˆë¬¸ìœ¼ë¡œ Self-RAGë¥¼ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.\n",
        "\n",
        "**ì£¼ìš” ë‚´ìš©:**\n",
        "- ì‚¬ì‹¤ì  ì§€ì‹ì´ í•„ìš” ì—†ëŠ” ì§ˆë¬¸ í…ŒìŠ¤íŠ¸\n",
        "- ê²€ìƒ‰ í•„ìš”ì„± í‰ê°€ì˜ ë™ì‘ í™•ì¸\n",
        "\n",
        "**ì‹¤í–‰ ê²°ê³¼:**\n",
        "- ê²€ìƒ‰ ì—†ì´ ì§ì ‘ ë‹µë³€ì´ ìƒì„±ë©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ê°„ë‹¨í•œ ì§ˆë¬¸ í…ŒìŠ¤íŠ¸\n",
        "simple_query = \"What is 2 + 2?\"\n",
        "print(\"\\n\\n\")\n",
        "self_rag(simple_query)\n",
        "\n",
        "print(\"\\nâœ… Self-RAG í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 7ï¸âƒ£ ìš”ì•½ ë° ë‹¤ìŒ ë‹¨ê³„\n",
        "\n",
        "### í•™ìŠµ ë‚´ìš© ìš”ì•½\n",
        "\n",
        "1. **Self-RAG ê°œë…**: ë‹¤ë‹¨ê³„ ìê¸° í‰ê°€ë¥¼ í†µí•œ ì‹ ë¢°ë„ ë†’ì€ RAG\n",
        "2. **í‰ê°€ ë‹¨ê³„**: ê²€ìƒ‰ í•„ìš”ì„± â†’ ê´€ë ¨ì„± â†’ ì§€ì›ë„ â†’ ìœ ìš©ì„±\n",
        "3. **íš¨ê³¼**: Hallucination ê°ì†Œ, ê·¼ê±° ê¸°ë°˜ ì‘ë‹µ, í’ˆì§ˆ ìë™ í‰ê°€\n",
        "4. **êµ¬í˜„**: Ollamaë¥¼ í™œìš©í•œ ë¡œì»¬ LLM ê¸°ë°˜ í‰ê°€ ì‹œìŠ¤í…œ\n",
        "\n",
        "### Self-RAGì˜ ì¥ì \n",
        "\n",
        "- âœ… ë¶ˆí•„ìš”í•œ ê²€ìƒ‰ ë°©ì§€ (íš¨ìœ¨ì„±)\n",
        "- âœ… ê´€ë ¨ ì—†ëŠ” ë¬¸ì„œ í•„í„°ë§ (ì •í™•ë„)\n",
        "- âœ… ê·¼ê±° ì—†ëŠ” hallucination ê°ì†Œ (ì‹ ë¢°ë„)\n",
        "- âœ… ì‘ë‹µ í’ˆì§ˆ ìë™ í‰ê°€ (íˆ¬ëª…ì„±)\n",
        "\n",
        "### Self-RAGì˜ ê³ ë ¤ì‚¬í•­\n",
        "\n",
        "- âš ï¸  ë‹¤ë‹¨ê³„ LLM í˜¸ì¶œë¡œ ì¸í•œ ë†’ì€ ë ˆì´í„´ì‹œ\n",
        "- âš ï¸  í‰ê°€ì˜ ì •í™•ë„ê°€ LLM ëŠ¥ë ¥ì— ì˜ì¡´\n",
        "- âš ï¸  ê°„ë‹¨í•œ ì§ˆë¬¸ì—ëŠ” ì˜¤ë²„í—¤ë“œ\n",
        "\n",
        "### ë‹¤ìŒ ë‹¨ê³„\n",
        "\n",
        "- **Chapter 7.5**: ëª¨ë“  ê¸°ë²• í†µí•© (CCH + HyDE + Reranking + Self-RAG)\n",
        "  - CCHë¡œ ì„ë² ë”©ëœ ì¸ë±ìŠ¤ ì‚¬ìš©\n",
        "  - HyDEë¡œ ì¿¼ë¦¬ í™•ì¥\n",
        "  - Rerankingìœ¼ë¡œ ë¬¸ì„œ ì¬ì •ë ¬\n",
        "  - Self-RAGë¡œ ì „ì²´ í’ˆì§ˆ ê´€ë¦¬\n",
        "\n",
        "### ì£¼ìš” í•¨ìˆ˜\n",
        "\n",
        "- `assess_retrieval_need(query)`: ê²€ìƒ‰ í•„ìš”ì„± í‰ê°€\n",
        "- `assess_relevance(query, document)`: ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€\n",
        "- `generate_response(query, context)`: ì‘ë‹µ ìƒì„±\n",
        "- `assess_support(response, context)`: ì§€ì›ë„ í‰ê°€\n",
        "- `assess_utility(query, response)`: ìœ ìš©ì„± í‰ê°€\n",
        "- `self_rag(query)`: ì „ì²´ Self-RAG í”„ë¡œì„¸ìŠ¤\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
