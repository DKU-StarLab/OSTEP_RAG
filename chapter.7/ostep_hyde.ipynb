{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 7.2: Hypothetical Document Embeddings (HyDE)\n",
        "\n",
        "이 노트북은 OSTEP RAG 시스템에 HyDE 기법을 적용하여 검색 품질을 개선하는 방법을 실습합니다.\n",
        "\n",
        "## 📚 학습 목표\n",
        "- HyDE 개념 이해: 쿼리를 가상의 답변 문서로 확장\n",
        "- 짧은 쿼리와 긴 문서 간의 의미적 거리 문제 해결\n",
        "- Ollama를 활용한 로컬 LLM 기반 HyDE 구현\n",
        "\n",
        "## 📋 실습 구성\n",
        "1) 설정 및 하이퍼파라미터 정의\n",
        "2) 기존 데이터 및 Ollama 설정\n",
        "3) HyDE 개념 설명\n",
        "4) 가상 문서 생성 함수 구현\n",
        "5) 비교 실험: 일반 검색 vs HyDE 검색\n",
        "6) 다양한 쿼리로 HyDE 효과 확인\n",
        "\n",
        "> **핵심 아이디어**: 짧은 쿼리 대신 \"답변을 포함하는 가상 문서\"를 생성하여 검색하면, 실제 문서와의 유사도가 높아져 검색 정확도가 향상됩니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1️⃣ 설정 및 하이퍼파라미터 정의\n",
        "\n",
        "이 셀에서는 경로, 모델, 검색 파라미터 등을 정의합니다.\n",
        "\n",
        "**주요 내용:**\n",
        "- 기존 산출물 경로 (청크 JSON, FAISS 인덱스)\n",
        "- Ollama 서버 설정 (로컬 LLM)\n",
        "- HyDE 관련 파라미터 (생성할 가상 문서 길이 등)\n",
        "\n",
        "**실행 결과:**\n",
        "- 설정값이 출력됩니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# 1️⃣ 설정 / 하이퍼파라미터\n",
        "# ========================================\n",
        "import os\n",
        "import json\n",
        "import faiss\n",
        "import numpy as np\n",
        "import torch\n",
        "import requests\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# 재현성\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# 디바이스\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# 경로 (기존 산출물)\n",
        "CHUNK_FILE = \"/home/kbs1102/workspace/OSTEP_RAG/data/chunk/ostep_tok400_ov20.json\"\n",
        "INDEX_FILE = \"/home/kbs1102/workspace/OSTEP_RAG/data/index/ostep_hnsw.index\"\n",
        "\n",
        "# 검색 하이퍼파라미터\n",
        "TOP_K = 5\n",
        "EF_SEARCH = 64\n",
        "\n",
        "# 임베딩 모델\n",
        "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "NORMALIZE = True\n",
        "\n",
        "# Ollama 설정 (HyDE용 LLM)\n",
        "OLLAMA_HOST = \"http://localhost:11434\"\n",
        "LLM_MODEL = \"llama3.1:8b\"\n",
        "TEMPERATURE = 0.3  # 낮은 temperature로 더 일관된 가상 문서 생성\n",
        "MAX_TOKENS = 300   # 가상 문서 길이\n",
        "\n",
        "print(f\"🔧 Device: {DEVICE}\")\n",
        "print(f\"📁 청크 파일: {CHUNK_FILE}\")\n",
        "print(f\"📁 인덱스 파일: {INDEX_FILE}\")\n",
        "print(f\"🔎 TOP_K={TOP_K}, efSearch={EF_SEARCH}\")\n",
        "print(f\"🧠 임베딩 모델: {EMBED_MODEL}\")\n",
        "print(f\"🤖 LLM: {LLM_MODEL} @ {OLLAMA_HOST}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2️⃣ 기존 데이터 및 Ollama 설정\n",
        "\n",
        "Chapter 2-4에서 생성한 청크 JSON과 FAISS 인덱스를 로드하고, Ollama 서버 연결을 확인합니다.\n",
        "\n",
        "**주요 내용:**\n",
        "- 청크 데이터 로드\n",
        "- FAISS 인덱스 로드\n",
        "- 임베딩 모델 로드\n",
        "- Ollama 서버 연결 확인\n",
        "\n",
        "**실행 결과:**\n",
        "- 데이터 로드 완료 메시지와 Ollama 서버 상태가 출력됩니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 청크 로드\n",
        "print(\"청크 데이터 로드 중...\")\n",
        "with open(CHUNK_FILE, 'r', encoding='utf-8') as f:\n",
        "    CHUNKS = json.load(f)\n",
        "print(f\"✓ 청크 로드 완료: {len(CHUNKS)}개\")\n",
        "\n",
        "# 인덱스 로드\n",
        "print(\"\\n인덱스 로드 중...\")\n",
        "INDEX = faiss.read_index(INDEX_FILE)\n",
        "print(f\"✓ 인덱스 로드 완료: {INDEX.ntotal}개 벡터\")\n",
        "\n",
        "# 임베딩 모델 로드\n",
        "print(f\"\\n임베딩 모델 로드 중: {EMBED_MODEL}\")\n",
        "from sentence_transformers import SentenceTransformer\n",
        "embed_model = SentenceTransformer(EMBED_MODEL, device=\"cpu\" if DEVICE==\"cpu\" else DEVICE)\n",
        "print(\"✓ 임베딩 모델 로드 완료\")\n",
        "\n",
        "# Ollama 서버 연결 확인\n",
        "print(f\"\\nOllama 서버 연결 확인: {OLLAMA_HOST}\")\n",
        "try:\n",
        "    response = requests.get(f\"{OLLAMA_HOST}/api/version\", timeout=5)\n",
        "    if response.status_code == 200:\n",
        "        print(f\"✓ Ollama 서버 연결 성공: {response.json()}\")\n",
        "    else:\n",
        "        print(f\"⚠️  Ollama 서버 응답 오류: {response.status_code}\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️  Ollama 서버 연결 실패: {e}\")\n",
        "    print(\"   Ollama 서버가 실행 중인지 확인하세요: ollama serve\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3️⃣ HyDE 개념 설명\n",
        "\n",
        "이 셀에서는 HyDE의 핵심 개념을 설명합니다.\n",
        "\n",
        "**HyDE란?**\n",
        "- **문제**: 짧은 쿼리 vs 긴 문서 사이의 의미적 거리가 큼\n",
        "  - 예: 쿼리 \"virtual memory\" vs 문서 \"Virtual memory is a memory management technique...\"\n",
        "- **해결책**: 쿼리를 \"답변을 포함하는 가상 문서\"로 확장\n",
        "  1. 쿼리를 받음: \"What is virtual memory?\"\n",
        "  2. LLM이 가상 답변 문서 생성: \"Virtual memory is a technique that...\"\n",
        "  3. 가상 문서를 임베딩하여 검색\n",
        "- **효과**: 가상 문서와 실제 문서가 더 유사한 형식이므로 검색 정확도 향상\n",
        "\n",
        "**HyDE의 장점:**\n",
        "- 쿼리-문서 간 형식 차이 해소\n",
        "- 복잡한 질문의 의도를 더 잘 포착\n",
        "- 다양한 표현 방식을 하나의 가상 문서로 통합\n",
        "\n",
        "**실행 결과:**\n",
        "- 개념 설명이 출력됩니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"[HyDE (Hypothetical Document Embeddings) 개념]\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "print(\"전통적 검색:\")\n",
        "print(\"  쿼리: 'What is virtual memory?' → 임베딩 → 검색\")\n",
        "print()\n",
        "print(\"HyDE 검색:\")\n",
        "print(\"  쿼리: 'What is virtual memory?'\")\n",
        "print(\"  ↓ LLM 생성\")\n",
        "print(\"  가상 문서: 'Virtual memory is a memory management technique...'\")\n",
        "print(\"  ↓ 임베딩\")\n",
        "print(\"  검색 (실제 문서와 형식이 유사하여 더 정확한 매칭)\")\n",
        "print()\n",
        "print(\"=\" * 80)\n",
        "print(\"💡 핵심: 짧은 질문보다 긴 답변 형식이 문서와 더 잘 매칭됩니다.\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4️⃣ 가상 문서 생성 함수 구현\n",
        "\n",
        "이 셀에서는 Ollama를 사용하여 쿼리로부터 가상 문서를 생성하는 함수를 구현합니다.\n",
        "\n",
        "**주요 내용:**\n",
        "- Ollama API 호출 함수\n",
        "- 가상 문서 생성 프롬프트 설계\n",
        "- 샘플 쿼리로 가상 문서 생성 테스트\n",
        "\n",
        "**실행 결과:**\n",
        "- 샘플 쿼리에 대한 가상 문서가 생성되어 출력됩니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ollama_generate(prompt: str, temperature: float = TEMPERATURE, max_tokens: int = MAX_TOKENS) -> str:\n",
        "    \"\"\"\n",
        "    Ollama를 사용하여 텍스트를 생성합니다.\n",
        "    \n",
        "    Args:\n",
        "        prompt: 생성할 프롬프트\n",
        "        temperature: 생성 temperature\n",
        "        max_tokens: 최대 토큰 수\n",
        "    \n",
        "    Returns:\n",
        "        생성된 텍스트\n",
        "    \"\"\"\n",
        "    payload = {\n",
        "        \"model\": LLM_MODEL,\n",
        "        \"prompt\": prompt,\n",
        "        \"stream\": False,\n",
        "        \"options\": {\n",
        "            \"temperature\": temperature,\n",
        "            \"num_predict\": max_tokens,\n",
        "        }\n",
        "    }\n",
        "    try:\n",
        "        r = requests.post(f\"{OLLAMA_HOST}/api/generate\", json=payload, timeout=60)\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "        return data.get(\"response\", \"\")\n",
        "    except Exception as e:\n",
        "        return f\"[Error] {e}\"\n",
        "\n",
        "def generate_hypothetical_document(query: str) -> str:\n",
        "    \"\"\"\n",
        "    쿼리로부터 가상 문서를 생성합니다.\n",
        "    \n",
        "    Args:\n",
        "        query: 사용자 질문\n",
        "    \n",
        "    Returns:\n",
        "        가상 답변 문서\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"Given the question below, write a detailed and informative passage that answers the question. \n",
        "The passage should be similar in style to an educational textbook or technical documentation.\n",
        "Do not include the question in your response, only provide the answer passage.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer passage:\"\"\"\n",
        "    \n",
        "    hypothetical_doc = ollama_generate(prompt)\n",
        "    return hypothetical_doc.strip()\n",
        "\n",
        "# 샘플 테스트\n",
        "print(\"=\" * 80)\n",
        "print(\"[가상 문서 생성 테스트]\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "test_query = \"How does the operating system handle virtual memory?\"\n",
        "print(f\"\\n쿼리: {test_query}\")\n",
        "print(\"\\n가상 문서 생성 중...\")\n",
        "hypothetical_doc = generate_hypothetical_document(test_query)\n",
        "print(f\"\\n생성된 가상 문서:\")\n",
        "print(\"-\" * 80)\n",
        "print(hypothetical_doc)\n",
        "print(\"-\" * 80)\n",
        "print(f\"\\n가상 문서 길이: {len(hypothetical_doc)} 글자\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5️⃣ 비교 실험: 일반 검색 vs HyDE 검색\n",
        "\n",
        "이 셀에서는 동일한 쿼리에 대해 일반 검색과 HyDE 검색을 비교합니다.\n",
        "\n",
        "**주요 내용:**\n",
        "- 일반 검색: 쿼리를 직접 임베딩하여 검색\n",
        "- HyDE 검색: 가상 문서를 생성하고 임베딩하여 검색\n",
        "- 결과 비교\n",
        "\n",
        "**실행 결과:**\n",
        "- 두 방식의 검색 결과와 유사도 점수가 출력됩니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def distance_to_similarity(distances: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"L2 거리를 유사도로 변환 (정규화된 벡터 가정)\"\"\"\n",
        "    return 1.0 - (distances / 2.0)\n",
        "\n",
        "def search_with_text(text: str, k: int = TOP_K):\n",
        "    \"\"\"텍스트를 임베딩하여 FAISS 인덱스에서 검색\"\"\"\n",
        "    faiss.ParameterSpace().set_index_parameter(INDEX, \"efSearch\", EF_SEARCH)\n",
        "    text_emb = embed_model.encode([text], convert_to_numpy=True, normalize_embeddings=NORMALIZE).astype('float32')\n",
        "    D, I = INDEX.search(text_emb, k)\n",
        "    scores = distance_to_similarity(D[0])\n",
        "    return I[0], scores\n",
        "\n",
        "# 테스트 쿼리\n",
        "TEST_QUERY = \"How does the operating system manage virtual memory?\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(f\"테스트 쿼리: {TEST_QUERY}\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# 1) 일반 검색\n",
        "print(\"\\n[1. 일반 검색 (쿼리 직접 사용)]\")\n",
        "print(\"-\" * 80)\n",
        "indices_normal, scores_normal = search_with_text(TEST_QUERY, k=3)\n",
        "for rank, (idx, score) in enumerate(zip(indices_normal, scores_normal), 1):\n",
        "    if idx >= 0:\n",
        "        chunk = CHUNKS[idx]\n",
        "        print(f\"\\n{rank}. Score: {score:.4f}\")\n",
        "        print(f\"   Chapter: {chunk.get('chapter_title', 'N/A')}\")\n",
        "        print(f\"   Preview: {chunk['text'][:120]}...\")\n",
        "\n",
        "# 2) HyDE 검색\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"[2. HyDE 검색 (가상 문서 사용)]\")\n",
        "print(\"-\" * 80)\n",
        "print(\"가상 문서 생성 중...\")\n",
        "hyde_doc = generate_hypothetical_document(TEST_QUERY)\n",
        "print(f\"✓ 가상 문서 생성 완료 ({len(hyde_doc)} 글자)\")\n",
        "print(f\"\\n가상 문서 미리보기:\\n{hyde_doc[:200]}...\\n\")\n",
        "\n",
        "indices_hyde, scores_hyde = search_with_text(hyde_doc, k=3)\n",
        "for rank, (idx, score) in enumerate(zip(indices_hyde, scores_hyde), 1):\n",
        "    if idx >= 0:\n",
        "        chunk = CHUNKS[idx]\n",
        "        print(f\"\\n{rank}. Score: {score:.4f}\")\n",
        "        print(f\"   Chapter: {chunk.get('chapter_title', 'N/A')}\")\n",
        "        print(f\"   Preview: {chunk['text'][:120]}...\")\n",
        "\n",
        "# 비교 결과\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"[비교 결과]\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"일반 검색 Top-1 점수: {scores_normal[0]:.4f}\")\n",
        "print(f\"HyDE 검색 Top-1 점수: {scores_hyde[0]:.4f}\")\n",
        "print(f\"\\n개선도: {(scores_hyde[0] - scores_normal[0]):.4f}\")\n",
        "print(\"\\n💡 HyDE는 가상 문서를 통해 쿼리를 더 풍부하게 표현하여\")\n",
        "print(\"   실제 문서와의 유사도를 높입니다.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6️⃣ 다양한 쿼리로 HyDE 효과 확인\n",
        "\n",
        "이 셀에서는 여러 쿼리로 HyDE의 효과를 추가 확인합니다.\n",
        "\n",
        "**주요 내용:**\n",
        "- 다양한 유형의 쿼리 테스트\n",
        "- 일반 검색 vs HyDE 검색 비교\n",
        "- 검색 결과 차이 분석\n",
        "\n",
        "**실행 결과:**\n",
        "- 각 쿼리별 검색 결과와 점수 개선도가 출력됩니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 추가 테스트 쿼리\n",
        "test_queries = [\n",
        "    \"What is the role of the scheduler?\",\n",
        "    \"Explain paging in memory management\",\n",
        "    \"How do processes synchronize?\"\n",
        "]\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"[추가 쿼리 테스트]\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "results = []\n",
        "\n",
        "for i, query in enumerate(test_queries, 1):\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"쿼리 {i}: {query}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    # 일반 검색\n",
        "    indices_normal, scores_normal = search_with_text(query, k=1)\n",
        "    \n",
        "    # HyDE 검색\n",
        "    print(\"가상 문서 생성 중...\")\n",
        "    hyde_doc = generate_hypothetical_document(query)\n",
        "    indices_hyde, scores_hyde = search_with_text(hyde_doc, k=1)\n",
        "    \n",
        "    print(f\"\\n일반 검색 Top-1:\")\n",
        "    print(f\"  Score: {scores_normal[0]:.4f}\")\n",
        "    print(f\"  Chapter: {CHUNKS[indices_normal[0]].get('chapter_title', 'N/A')}\")\n",
        "    \n",
        "    print(f\"\\nHyDE 검색 Top-1:\")\n",
        "    print(f\"  Score: {scores_hyde[0]:.4f}\")\n",
        "    print(f\"  Chapter: {CHUNKS[indices_hyde[0]].get('chapter_title', 'N/A')}\")\n",
        "    \n",
        "    improvement = scores_hyde[0] - scores_normal[0]\n",
        "    print(f\"\\n개선도: {improvement:+.4f} ({'↑' if improvement > 0 else '↓' if improvement < 0 else '→'})\")\n",
        "    \n",
        "    results.append({\n",
        "        'query': query,\n",
        "        'normal_score': scores_normal[0],\n",
        "        'hyde_score': scores_hyde[0],\n",
        "        'improvement': improvement\n",
        "    })\n",
        "\n",
        "# 전체 요약\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"[전체 요약]\")\n",
        "print(\"=\" * 80)\n",
        "avg_improvement = np.mean([r['improvement'] for r in results])\n",
        "print(f\"평균 개선도: {avg_improvement:+.4f}\")\n",
        "print(f\"개선된 쿼리: {sum(1 for r in results if r['improvement'] > 0)}/{len(results)}\")\n",
        "print(\"\\n✅ HyDE 테스트 완료!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 7️⃣ 요약 및 다음 단계\n",
        "\n",
        "### 학습 내용 요약\n",
        "\n",
        "1. **HyDE 개념**: 쿼리를 가상의 답변 문서로 확장하여 검색\n",
        "2. **효과**: 쿼리-문서 간 형식 차이를 해소하여 검색 정확도 향상\n",
        "3. **구현**: Ollama를 활용한 로컬 LLM 기반 가상 문서 생성\n",
        "4. **검증**: 동일 쿼리에 대해 일반 검색보다 높은 유사도 점수 확인\n",
        "\n",
        "### HyDE의 장점\n",
        "\n",
        "- ✅ 짧은 쿼리와 긴 문서 간 의미적 거리 해소\n",
        "- ✅ 복잡한 질문의 의도를 더 잘 포착\n",
        "- ✅ 다양한 표현 방식을 통합\n",
        "- ✅ 추가 인덱싱 불필요 (런타임에만 적용)\n",
        "\n",
        "### HyDE의 고려사항\n",
        "\n",
        "- ⚠️  LLM 호출로 인한 레이턴시 증가\n",
        "- ⚠️  가상 문서가 실제 답변과 다를 수 있음 (hallucination 가능)\n",
        "- ⚠️  짧고 간단한 쿼리에는 오버헤드\n",
        "\n",
        "### 다음 단계\n",
        "\n",
        "- **Chapter 7.3**: Reranking (검색 결과 재정렬)\n",
        "- **Chapter 7.4**: Self-RAG (자기 평가 기반 검색)\n",
        "- **Chapter 7.5**: 모든 기법 통합 (CCH + HyDE + Reranking + Self-RAG)\n",
        "\n",
        "### 주요 함수\n",
        "\n",
        "- `generate_hypothetical_document(query)`: 쿼리로부터 가상 문서 생성\n",
        "- `search_with_text(text, k)`: 텍스트를 임베딩하여 검색\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
