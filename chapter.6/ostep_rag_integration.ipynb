{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a0760dc",
   "metadata": {},
   "source": [
    "# Chapter 6: OSTEP RAG í†µí•© (ê²€ìƒ‰â†’í”„ë¡¬í”„íŠ¸â†’ìƒì„±)\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ Chapter 2â€“5ì—ì„œ ìƒì„±í•œ ì‚°ì¶œë¬¼(ì²­í¬/ì„ë² ë”©/FAISS ì¸ë±ìŠ¤)ì„ ì¬ì‚¬ìš©í•˜ì—¬, ê²€ìƒ‰â†’í”„ë¡¬í”„íŠ¸â†’ìƒì„±ê¹Œì§€ ë‹¨ì¼ RAG íŒŒì´í”„ë¼ì¸ì„ ì‹œì—°í•©ë‹ˆë‹¤.\n",
    "\n",
    "## ğŸ“š í•™ìŠµ ëª©í‘œ\n",
    "- ê¸°ì¡´ ì‚°ì¶œë¬¼ ë¡œë“œë§Œìœ¼ë¡œ RAG í†µí•© íŒŒì´í”„ë¼ì¸ êµ¬ì„±\n",
    "- FAISS HNSW ì¸ë±ìŠ¤ë¥¼ ì´ìš©í•œ Top-K ê²€ìƒ‰(Top-K=10)\n",
    "- Ollama `llama3.1`ë¡œ êµ¬ì¡°í™” í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ ì‘ë‹µ ìƒì„±\n",
    "\n",
    "## ğŸ“‹ ì‹¤ìŠµ êµ¬ì„±\n",
    "1) ì„¤ì •/í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ì˜(ê²½ë¡œ/ëª¨ë¸/ê²€ìƒ‰ íŒŒë¼ë¯¸í„°)\n",
    "2) ì‚°ì¶œë¬¼ ë¡œë“œ(ì²­í¬ JSON, ì¸ë±ìŠ¤/ë©”íƒ€)\n",
    "3) ì„ë² ë”© ëª¨ë¸ ë¡œë“œ(`all-MiniLM-L6-v2`)\n",
    "4) ê²€ìƒ‰ í•¨ìˆ˜ ì •ì˜(retrieve)\n",
    "5) í”„ë¡¬í”„íŠ¸/ìƒì„±(í—¬í¼ í•¨ìˆ˜)\n",
    "6) í†µí•© í•¨ìˆ˜(rag_answer)\n",
    "7) ë°ëª¨ ì‹¤í–‰(ì§ˆë¬¸â†’ë‹µë³€+ì¶œì²˜)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60435f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Device: cuda\n",
      "ğŸ“ Index: /home/kbs1102/workspace/OSTEP_RAG/data/index/ostep_hnsw.index\n",
      "ğŸ“ Chunks: /home/kbs1102/workspace/OSTEP_RAG/data/chunk/ostep_tok400_ov20.json\n",
      "ğŸ” TOP_K=10, efSearch=64\n",
      "ğŸ§  Embed: sentence-transformers/all-MiniLM-L6-v2\n",
      "ğŸ¤– LLM: llama3.1:8b @ http://localhost:11434\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 1ï¸âƒ£ ì„¤ì • / í•˜ì´í¼íŒŒë¼ë¯¸í„° / ì˜ì¡´ì„± ì„í¬íŠ¸\n",
    "#   - ê²½ë¡œ/ëª¨ë¸/ê²€ìƒ‰ íŒŒë¼ë¯¸í„°ëŠ” ìƒë‹¨ ë³€ìˆ˜ì—ì„œ í†µì¼ ê´€ë¦¬\n",
    "# ========================================\n",
    "import os\n",
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "import requests\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# ì¬í˜„ì„±\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# ë””ë°”ì´ìŠ¤\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ê²½ë¡œ (Chapter 4 ì‚°ì¶œë¬¼ ê¸°ì¤€)\n",
    "INDEX_FILE = \"/home/kbs1102/workspace/OSTEP_RAG/data/index/ostep_hnsw.index\"\n",
    "INDEX_META_FILE = \"/home/kbs1102/workspace/OSTEP_RAG/data/index/ostep_hnsw_metadata.json\"\n",
    "CHUNK_FILE = \"/home/kbs1102/workspace/OSTEP_RAG/data/chunk/ostep_tok400_ov20.json\"\n",
    "\n",
    "# ê²€ìƒ‰ í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "TOP_K = 10\n",
    "EF_SEARCH = 64           # HNSW efSearch (ê²€ìƒ‰ í’ˆì§ˆ/ì†ë„ íŠ¸ë ˆì´ë“œì˜¤í”„)\n",
    "MIN_SCORE = 0.0          # í•„í„° ì„ê³„ê°’\n",
    "\n",
    "# ì„ë² ë”© ëª¨ë¸ (ì¿¼ë¦¬ ì „ìš©)\n",
    "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "NORMALIZE = True         # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ ì ìˆ˜í™”ë¥¼ ìœ„í•´ ì •ê·œí™” ì‚¬ìš©\n",
    "\n",
    "# LLM(Ollama) í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "OLLAMA_HOST = \"http://localhost:11434\"\n",
    "LLM_MODEL = \"llama3.1:8b\"\n",
    "TEMPERATURE = 0.2\n",
    "TOP_P = 0.9\n",
    "MAX_TOKENS = 512\n",
    "REPEAT_PENALTY = 1.1\n",
    "\n",
    "print(f\"ğŸ”§ Device: {DEVICE}\")\n",
    "print(f\"ğŸ“ Index: {INDEX_FILE}\")\n",
    "print(f\"ğŸ“ Chunks: {CHUNK_FILE}\")\n",
    "print(f\"ğŸ” TOP_K={TOP_K}, efSearch={EF_SEARCH}\")\n",
    "print(f\"ğŸ§  Embed: {EMBED_MODEL}\")\n",
    "print(f\"ğŸ¤– LLM: {LLM_MODEL} @ {OLLAMA_HOST}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8c3723",
   "metadata": {},
   "source": [
    "---\n",
    "## 2ï¸âƒ£ ì‚°ì¶œë¬¼ ë¡œë“œ\n",
    "\n",
    "Chapter 2â€“4ì—ì„œ ìƒì„±í•œ ì²­í¬ JSONê³¼ FAISS ì¸ë±ìŠ¤/ë©”íƒ€ ì •ë³´ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a668a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì²­í¬ ë¡œë“œ\n",
    "with open(CHUNK_FILE, 'r', encoding='utf-8') as f:\n",
    "    CHUNKS = json.load(f)\n",
    "print(f\"âœ“ Chunks loaded: {len(CHUNKS)} entries\")\n",
    "\n",
    "# ì¸ë±ìŠ¤/ë©”íƒ€ ë¡œë“œ\n",
    "INDEX = faiss.read_index(INDEX_FILE)\n",
    "print(f\"âœ“ Index loaded: {INDEX.ntotal} vectors\")\n",
    "\n",
    "with open(INDEX_META_FILE, 'r', encoding='utf-8') as f:\n",
    "    INDEX_META = json.load(f)\n",
    "print(\"Index metadata:\")\n",
    "for k, v in INDEX_META.items():\n",
    "    print(f\"  - {k}: {v}\")\n",
    "\n",
    "# efSearch ì ìš©\n",
    "faiss.ParameterSpace().set_index_parameter(INDEX, \"efSearch\", EF_SEARCH)\n",
    "print(f\"efSearch set to {EF_SEARCH}\")\n",
    "\n",
    "# ìƒ˜í”Œ ì¶œë ¥\n",
    "print(\"\\nSample chunk:\")\n",
    "sample = CHUNKS[0]\n",
    "for key, value in sample.items():\n",
    "    if key == 'text':\n",
    "        print(f\"  {key}: {value[:100]}...\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2b40f8",
   "metadata": {},
   "source": [
    "---\n",
    "## 3ï¸âƒ£ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ì¿¼ë¦¬ ì „ìš©)\n",
    "\n",
    "`sentence-transformers/all-MiniLM-L6-v2`ë¥¼ ë¡œë“œí•˜ì—¬ ì¿¼ë¦¬ë¥¼ ì„ë² ë”©í•©ë‹ˆë‹¤. ê²€ìƒ‰ ì ìˆ˜ëŠ” ì •ê·œí™”ëœ L2 ê±°ë¦¬ë¡œë¶€í„° ìœ ì‚¬ë„(â‰ˆ 1 - distance/2)ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28a3fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "print(f\"Loading model: {EMBED_MODEL} on {DEVICE}\")\n",
    "embed_model = SentenceTransformer(EMBED_MODEL, device=\"cpu\" if DEVICE==\"cpu\" else DEVICE)\n",
    "print(\"âœ“ Embed model ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85407e6",
   "metadata": {},
   "source": [
    "---\n",
    "## 4ï¸âƒ£ ê²€ìƒ‰ í•¨ìˆ˜ ì •ì˜\n",
    "\n",
    "ì¿¼ë¦¬ â†’ ì„ë² ë”© â†’ FAISS HNSW ê²€ìƒ‰ â†’ ì ìˆ˜ ë³€í™˜ â†’ ìƒìœ„ Kê°œì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa76eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class RetrievedChunk:\n",
    "    index: int\n",
    "    score: float\n",
    "    chunk: Dict[str, Any]\n",
    "\n",
    "def _distance_to_similarity(distances: np.ndarray) -> np.ndarray:\n",
    "    # ì •ê·œí™”ëœ ë²¡í„°ì˜ L2 ê±°ë¦¬ â†’ ìœ ì‚¬ë„ ê·¼ì‚¬: 1 - d/2\n",
    "    return 1.0 - (distances / 2.0)\n",
    "\n",
    "def retrieve(query: str, k: int = TOP_K, ef_search: int = EF_SEARCH, min_score: float = MIN_SCORE) -> List[RetrievedChunk]:\n",
    "    faiss.ParameterSpace().set_index_parameter(INDEX, \"efSearch\", ef_search)\n",
    "    q_emb = embed_model.encode([query], convert_to_numpy=True, normalize_embeddings=NORMALIZE).astype('float32')\n",
    "    D, I = INDEX.search(q_emb, k)\n",
    "    scores = _distance_to_similarity(D[0])\n",
    "\n",
    "    results: List[RetrievedChunk] = []\n",
    "    for rank, (idx, sc) in enumerate(zip(I[0], scores)):\n",
    "        if idx < 0:\n",
    "            continue\n",
    "        if sc < min_score:\n",
    "            continue\n",
    "        results.append(RetrievedChunk(index=idx, score=float(sc), chunk=CHUNKS[idx]))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209f3295",
   "metadata": {},
   "source": [
    "---\n",
    "## 5ï¸âƒ£ í”„ë¡¬í”„íŠ¸/ìƒì„± í—¬í¼ í•¨ìˆ˜\n",
    "\n",
    "Chapter 5 ìŠ¤íƒ€ì¼ì˜ `ollama_generate()`ì™€ `build_structured_prompt()`ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93ef7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ollama_generate(prompt: str) -> str:\n",
    "    payload = {\n",
    "        \"model\": LLM_MODEL,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"temperature\": TEMPERATURE,\n",
    "            \"top_p\": TOP_P,\n",
    "            \"num_predict\": MAX_TOKENS,\n",
    "            \"repeat_penalty\": REPEAT_PENALTY,\n",
    "        }\n",
    "    }\n",
    "    try:\n",
    "        r = requests.post(f\"{OLLAMA_HOST}/api/generate\", json=payload, timeout=600)\n",
    "        r.raise_for_status()\n",
    "        data = r.json() if isinstance(r.json(), dict) else json.loads(r.text)\n",
    "        return data.get(\"response\", \"\")\n",
    "    except Exception as e:\n",
    "        return f\"[Error] {e}\"\n",
    "\n",
    "\n",
    "def build_structured_prompt(\n",
    "    role: str,\n",
    "    goal: str,\n",
    "    constraints: str,\n",
    "    format_spec: str,\n",
    "    context: List[str],\n",
    "    question: str,\n",
    ") -> str:\n",
    "    context_text = \"\\n\\n\".join([f\"[Context {i+1}]\\n{chunk}\" for i, chunk in enumerate(context)])\n",
    "    prompt = f\"\"\"Role:\n",
    "{role}\n",
    "\n",
    "Goal:\n",
    "{goal}\n",
    "\n",
    "Constraints:\n",
    "{constraints}\n",
    "\n",
    "Format:\n",
    "{format_spec}\n",
    "\n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Please answer the question considering the role, goal, constraints, format, and context provided above.\"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4168bb4",
   "metadata": {},
   "source": [
    "---\n",
    "## 6ï¸âƒ£ í†µí•© RAG í•¨ìˆ˜\n",
    "\n",
    "ê²€ìƒ‰ â†’ ì»¨í…ìŠ¤íŠ¸ ì„ íƒ â†’ êµ¬ì¡°í™” í”„ë¡¬í”„íŠ¸ ìƒì„± â†’ Ollama í˜¸ì¶œ â†’ ì‘ë‹µ/ì¶œì²˜ ë°˜í™˜.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea47c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _format_context_from_results(results: List[RetrievedChunk], max_chars: int = 600) -> List[str]:\n",
    "    formatted = []\n",
    "    for r in results:\n",
    "        text = r.chunk.get(\"text\", \"\")\n",
    "        snippet = text[:max_chars]\n",
    "        title = r.chunk.get(\"chapter_title\") or r.chunk.get(\"chapter_id\") or \"\"\n",
    "        prefix = f\"[Chapter: {title}]\\nScore: {r.score:.4f}\\n\"\n",
    "        formatted.append(prefix + snippet)\n",
    "    return formatted\n",
    "\n",
    "ROLE = \"\"\"You are an expert tutor specializing in operating system concepts.\n",
    "You explain concepts clearly and concisely so students can understand easily,\n",
    "and you use concrete examples to illustrate concepts.\"\"\"\n",
    "\n",
    "GOAL = \"\"\"Provide evidence-based answers to questions based on the provided context.\n",
    "Find and cite relevant information from the context in your answers,\n",
    "and explicitly state when information is not available in the context.\"\"\"\n",
    "\n",
    "CONSTRAINTS = \"\"\"1. Your answer must be based on the provided context.\n",
    "2. If you can find relevant information in the context, you must include 1-3 citations.\n",
    "3. If relevant information is not available in the context, you must explicitly state 'The information is not available in the provided context.'\n",
    "4. Any speculative or uncertain content must be clearly marked.\"\"\"\n",
    "\n",
    "FORMAT_SPEC = \"\"\"Please respond in the following JSON format:\n",
    "\n",
    "{\n",
    "  \"answer\": \"Answer to the question (1-2 paragraphs)\",\n",
    "  \"citations\": [\n",
    "    {\"chunk_id\": 1, \"quote\": \"quoted text\"},\n",
    "    {\"chunk_id\": 2, \"quote\": \"quoted text\"}\n",
    "  ],\n",
    "  \"summary\": \"Summary (1-2 sentences)\"\n",
    "}\"\"\"\n",
    "\n",
    "def rag_answer(question: str, top_k: int = TOP_K) -> Dict[str, Any]:\n",
    "    # 1) Retrieve\n",
    "    results = retrieve(question, k=top_k, ef_search=EF_SEARCH, min_score=MIN_SCORE)\n",
    "\n",
    "    # 2) Build prompt (use top-3 for compactness)\n",
    "    top_context = _format_context_from_results(results[:3])\n",
    "    prompt = build_structured_prompt(\n",
    "        role=ROLE,\n",
    "        goal=GOAL,\n",
    "        constraints=CONSTRAINTS,\n",
    "        format_spec=FORMAT_SPEC,\n",
    "        context=top_context,\n",
    "        question=question,\n",
    "    )\n",
    "\n",
    "    # 3) Generate\n",
    "    answer = ollama_generate(prompt)\n",
    "\n",
    "    # 4) Sources\n",
    "    sources = []\n",
    "    for r in results[:3]:\n",
    "        preview = (r.chunk.get(\"text\", \"\")[:80] + \"...\") if r.chunk.get(\"text\") else \"\"\n",
    "        sources.append({\n",
    "            \"score\": round(r.score, 4),\n",
    "            \"chapter_title\": r.chunk.get(\"chapter_title\"),\n",
    "            \"chunk_id\": r.chunk.get(\"chunk_id\"),\n",
    "            \"preview\": preview,\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"top_k\": top_k,\n",
    "        \"sources\": sources,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9a3628",
   "metadata": {},
   "source": [
    "---\n",
    "## 7ï¸âƒ£ ë°ëª¨ ì‹¤í–‰: ì§ˆë¬¸ â†’ ë‹µë³€ + ì¶œì²˜\n",
    "\n",
    "`rag_answer()`ë¥¼ í˜¸ì¶œí•´ í†µí•© ë™ì‘ì„ í™•ì¸í•©ë‹ˆë‹¤. Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì´ì–´ì•¼ í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367ca69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How does the operating system handle memory virtualization?\"\n",
    "result = rag_answer(question)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Question:\")\n",
    "print(result[\"question\"]) \n",
    "print(\"=\"*80)\n",
    "print(\"Answer:\")\n",
    "print(result[\"answer\"]) \n",
    "print(\"\\n[Sources]\")\n",
    "for i, s in enumerate(result[\"sources\"], 1):\n",
    "    print(f\"{i}. ({s['score']:.4f}) [{s.get('chapter_title')}] {s.get('preview')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a562a5d",
   "metadata": {},
   "source": [
    "---\n",
    "## 8ï¸âƒ£ (ì„ íƒ) ê°„ë‹¨ ê²€ìƒ‰ ì í•©ì„± í™•ì¸\n",
    "\n",
    "`test_queries.json`ì—ì„œ ì¼ë¶€ ì§ˆì˜ë¥¼ ë¶ˆëŸ¬ì™€ ê²€ìƒ‰ ìƒìœ„ ê²°ê³¼ì˜ ì±•í„° íƒ€ì´í‹€ì„ ë¯¸ë¦¬ë³´ê¸°ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71198c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_QUERIES = \"/home/kbs1102/workspace/OSTEP_RAG/data/documents/test_queries.json\"\n",
    "\n",
    "try:\n",
    "    with open(TEST_QUERIES, 'r', encoding='utf-8') as f:\n",
    "        test_qs = json.load(f)\n",
    "    print(f\"Loaded {len(test_qs)} test queries. Showing first 3...\")\n",
    "    for q in test_qs[:3]:\n",
    "        rs = retrieve(q, k=5)\n",
    "        print(\"\\nQ:\", q)\n",
    "        for i, r in enumerate(rs, 1):\n",
    "            title = r.chunk.get('chapter_title')\n",
    "            print(f\"  {i}. {title} (score={r.score:.4f})\")\n",
    "except FileNotFoundError:\n",
    "    print(\"[Info] test_queries.json not found. Skipping optional check.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa4bf5e",
   "metadata": {},
   "source": [
    "---\n",
    "## 9ï¸âƒ£ ì¸í„°ë™í‹°ë¸Œ Q&A (ì‹¤ì‹œê°„ ì§ˆì˜)\n",
    "\n",
    "ì‚¬ìš©ìë¡œë¶€í„° ì…ë ¥ì„ ë°›ì•„ `rag_answer()`ë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤. ì¢…ë£Œí•˜ë ¤ë©´ `exit`ë¥¼ ì…ë ¥í•˜ì„¸ìš”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1372cb6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— \n",
      "â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—    â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â• \n",
      "â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ–ˆâ•—\n",
      "â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â•â•     â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘\n",
      "â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘         â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•\n",
      " â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â•â•   â•šâ•â•   â•šâ•â•â•â•â•â•â•â•šâ•â•         â•šâ•â•  â•šâ•â•â•šâ•â•  â•šâ•â• â•šâ•â•â•â•â•â• \n",
      "\n",
      "\t\t\t\t             By DKU System Software Lab\n",
      "\n",
      "ğŸ§  Embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "ğŸ¤– Language model: llama3.1:8b\n",
      "ğŸ“‡ Index type: HNSW\n",
      "=======================================================================\n",
      "\n",
      "Tips for getting started:\n",
      " 1. Ask questions about operating systems concepts\n",
      " 2. Be specific for the best results\n",
      " 3. Type 'exit' to quit the system\n",
      " 4. The system uses OSTEP textbook as knowledge base\n",
      "\n",
      "=======================================================================\n",
      "\n",
      "Question:\n",
      "\n",
      "[Info] Empty input. Try again.\n",
      "[Info] Bye.\n"
     ]
    }
   ],
   "source": [
    "INIT_SCREEN = f'''\n",
    "\n",
    " â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— \n",
    "â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—    â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â• \n",
    "â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ–ˆâ•—\n",
    "â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â•šâ•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â•â•     â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘\n",
    "â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘         â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•\n",
    " â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â•â•   â•šâ•â•   â•šâ•â•â•â•â•â•â•â•šâ•â•         â•šâ•â•  â•šâ•â•â•šâ•â•  â•šâ•â• â•šâ•â•â•â•â•â• \n",
    "\n",
    "\t\t\t\t             By DKU System Software Lab\n",
    "\n",
    "ğŸ§  Embedding model: {EMBED_MODEL}\n",
    "ğŸ¤– Language model: {LLM_MODEL}\n",
    "ğŸ“‡ Index type: HNSW\n",
    "=======================================================================\n",
    "\n",
    "Tips for getting started:\n",
    " 1. Ask questions about operating systems concepts\n",
    " 2. Be specific for the best results\n",
    " 3. Type 'exit' to quit the system\n",
    " 4. The system uses OSTEP textbook as knowledge base\n",
    " \n",
    "=======================================================================\n",
    "\n",
    "Question:\n",
    "'''\n",
    "\n",
    "print(INIT_SCREEN)\n",
    "try:\n",
    "    while True:\n",
    "        user_q = input(\"Enter your question (type 'exit' to quit): \").strip()\n",
    "        if not user_q:\n",
    "            print(\"[Info] Empty input. Try again.\")\n",
    "            continue\n",
    "        if user_q.lower() in {\"exit\", \"quit\"}:\n",
    "            print(\"[Info] Bye.\")\n",
    "            break\n",
    "        result = rag_answer(user_q)\n",
    "        print(\"=\"*80)\n",
    "        print(\"Question:\")\n",
    "        print(result[\"question\"]) \n",
    "        print(\"=\"*80)\n",
    "        print(\"Answer:\")\n",
    "        print(result[\"answer\"]) \n",
    "        print(\"\\n[Sources]\")\n",
    "        for i, s in enumerate(result.get(\"sources\", []), 1):\n",
    "            print(f\"{i}. ({s['score']:.4f}) [{s.get('chapter_title')}] {s.get('preview')}\")\n",
    "        print(\"\\n\")\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n[Info] Interrupted by user.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
